{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torchsummary import summary\n",
    "import os\n",
    "from utils.dataloaders import get_dataloader, get_subnet_dataloader\n",
    "from utils.train_eval import evaluate\n",
    "from utils.functions import reconstruction_model\n",
    "from utils.io import load_weights\n",
    "\n",
    "from utils.train_eval import get_accuracy\n",
    "from utils.utils import count_net_flops\n",
    "\n",
    "from bn_fold import bn_fold\n",
    "from torch import nn\n",
    "from fxpmath import Fxp\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ckpt = \"./weights/mcu_vggrepc1_vww.pth\"\n",
    "model_ckpt = \"./weights/mcu_vggrepopt_cifar10.pth\"\n",
    "# data_dir = \"E:/1_TinyML/tiny/benchmark/training/visual_wake_words/vw_coco2014_96\"\n",
    "image_size = 32\n",
    "workers = 4\n",
    "batch_size = 50\n",
    "from models import MCU_VGGRep, MCU_VGGRepC1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCU_VGGRepC1(\n",
      "  (quant): QuantStub()\n",
      "  (STAGE0_CONV): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE0_RELU): ReLU()\n",
      "  (STAGE1_0_CONV): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE1_0_RELU): ReLU()\n",
      "  (STAGE2_0_CONV): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE2_0_RELU): ReLU()\n",
      "  (STAGE3_0_CONV): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE3_0_RELU): ReLU()\n",
      "  (STAGE4_0_CONV): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE4_0_RELU): ReLU()\n",
      "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
      "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
      "  (LINEAR): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MCU_VGGRepC1(num_classes=10)\n",
    "model = load_weights(model, model_ckpt)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading cifar10 data...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flatten_model = reconstruction_model(model, device)\n",
    "def get_cifar10_loader():\n",
    "    print('=> loading cifar10 data...')\n",
    "    normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='E:/2_Quantization/torch2cmsis/examples/cifar/data/data_cifar10',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ]))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='E:/2_Quantization/torch2cmsis/examples/cifar/data/data_cifar10',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ]))\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_cifar10_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating Point Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before accuracy: 75.94% MAC+BN=480,640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "\n",
    "print(f\"Before accuracy: {get_accuracy(model.to(device), testloader):.2f}%\",\n",
    "        f\"MAC+BN={count_net_flops(model, (1, 3, image_size, image_size), False):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cofing Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import get_default_qconfig, quantize_dynamic, QConfig, HistogramObserver, prepare, fuse_modules\n",
    "# Custom quantization configuration 정의\n",
    "import math\n",
    "from torch.quantization import MinMaxObserver\n",
    "import torch\n",
    "\n",
    "\n",
    "class PowerOfTwoActivationObserver(HistogramObserver):\n",
    "    def __init__(self, bits=8, *args, **kwargs):\n",
    "        super(PowerOfTwoActivationObserver, self).__init__(*args, **kwargs)\n",
    "        self.bits = bits\n",
    "        self.quant_min = 0\n",
    "        self.quant_max = 2 ** bits - 1\n",
    "        self.dtype = torch.qint8  # 추가: dtype을 torch.quint8로 설정\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0))\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        min_val, max_val = self.min_val, self.max_val\n",
    "        if min_val == max_val:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            max_val = max(abs(min_val), abs(max_val))\n",
    "            scale = max_val / ((2 ** self.bits) - 1)\n",
    "            scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "            zero_point = 0\n",
    "\n",
    "        self.scale.copy_(scale_pow2)\n",
    "        self.zero_point.copy_(zero_point)\n",
    "        return self.scale, self.zero_point\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.min_val = min(self.min_val, x.min())\n",
    "        self.max_val = max(self.max_val, x.max())\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"min_val={}, max_val={}, scale={}, zero_point={}, bits={}\".format(\n",
    "            self.min_val, self.max_val, self.scale, self.zero_point, self.bits\n",
    "        )\n",
    "class PowerOfTwoWeightObserver(HistogramObserver):\n",
    "    def __init__(self, bits=8, *args, **kwargs):\n",
    "        super(PowerOfTwoWeightObserver, self).__init__(*args, **kwargs)\n",
    "        self.bits = bits\n",
    "        self.dtype = torch.qint8  # 추가: dtype을 torch.quint8로 설정\n",
    "\n",
    "        self.quant_min = -2 ** (bits - 1)\n",
    "        self.quant_max = 2 ** (bits - 1) - 1\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0))\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        min_val, max_val = self.min_val, self.max_val\n",
    "        if min_val == max_val:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            max_val = max(abs(min_val), abs(max_val))\n",
    "            scale = max_val / (2 ** (self.bits - 1) - 1)\n",
    "            scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "            zero_point = 0\n",
    "\n",
    "        self.scale.copy_(scale_pow2)\n",
    "        self.zero_point.copy_(zero_point)\n",
    "        return self.scale, self.zero_point\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.min_val = min(self.min_val, x.min())\n",
    "        self.max_val = max(self.max_val, x.max())\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"min_val={}, max_val={}, scale={}, zero_point={}, bits={}\".format(\n",
    "            self.min_val, self.max_val, self.scale, self.zero_point, self.bits\n",
    "        )\n",
    "\n",
    "backend = \"x86\"\n",
    "model.qconfig = QConfig(\n",
    "            activation=PowerOfTwoWeightObserver.with_args(bits=8, \n",
    "                                                          qscheme=torch.per_tensor_symmetric,\n",
    "                                                          dtype=torch.qint8),\n",
    "            weight=PowerOfTwoWeightObserver.with_args(bits=8,\n",
    "                                              qscheme=torch.per_tensor_symmetric,\n",
    "                                              dtype=torch.qint8)\n",
    "            )\n",
    "fuse_modules(model, [['STAGE0_CONV', 'STAGE0_RELU'],\n",
    "                     ['STAGE1_0_CONV', 'STAGE1_0_RELU'],\n",
    "                     ['STAGE2_0_CONV', 'STAGE2_0_RELU'],\n",
    "                     ['STAGE3_0_CONV', 'STAGE3_0_RELU'],\n",
    "                     ['STAGE4_0_CONV', 'STAGE4_0_RELU']], inplace=True)\n",
    "# model.qconfig = torch.quantization.QConfig(\n",
    "#     activation=HistogramObserver.with_args(dtype=torch.qint8, \n",
    "#                                            qscheme=torch.per_tensor_symmetric),\n",
    "#     weight=HistogramObserver.with_args(dtype=torch.qint8,\n",
    "#                                         qscheme=torch.per_tensor_symmetric)\n",
    "# )\n",
    "qmodel = prepare(model, inplace=False)\n",
    "# model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "# torch.backends.quantized.engine = backend\n",
    "# model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "# model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n",
    "# model.qconfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCU_VGGRepC1(\n",
       "  (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
       "  (STAGE0_CONV): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE0_RELU): Identity()\n",
       "  (STAGE1_0_CONV): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE1_0_RELU): Identity()\n",
       "  (STAGE2_0_CONV): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE2_0_RELU): Identity()\n",
       "  (STAGE3_0_CONV): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.25, zero_point=0, padding=(1, 1))\n",
       "  (STAGE3_0_RELU): Identity()\n",
       "  (STAGE4_0_CONV): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.03125, zero_point=0, padding=(1, 1))\n",
       "  (STAGE4_0_RELU): Identity()\n",
       "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
       "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
       "  (LINEAR): QuantizedLinear(in_features=128, out_features=10, scale=0.03125, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "qmodel = qmodel.to(device)\n",
    "with torch.inference_mode():\n",
    "    for img, label in trainloader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        if cnt > 10:\n",
    "            break\n",
    "        qmodel(img)\n",
    "    \n",
    "qmodel = torch.quantization.convert(qmodel, inplace=True)\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(qmodel, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "imgs = iter(testloader).__next__()[0]\n",
    "print(imgs.shape)\n",
    "_ = qmodel(imgs.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0625, -0.9688, -0.0625,  1.9375, -0.9688,  1.2188,  0.1250, -1.5000,\n",
       "         -0.9062, -0.6875]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = imgs[0]\n",
    "img.shape\n",
    "qmodel(img.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0625, -0.9688, -0.0625,  1.9375, -0.9688,  1.2188,  0.1250, -1.5000,\n",
       "         -0.9062, -0.6875]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from fxpmath import Fxp\n",
    "import numpy as np\n",
    "\n",
    "def qfmt_quanize(x, n_bits=8, signed=True):\n",
    "    range_min, range_max = torch.min(x), torch.max(x)\n",
    "    range_abs = torch.max(torch.abs(range_min), torch.abs(range_max))\n",
    "    int_bits = torch.ceil(torch.log2(range_abs)).type(torch.int8)\n",
    "    frac_bits = n_bits - int_bits\n",
    "    if signed:\n",
    "        range_int_min = -(2 ** n_bits)\n",
    "        range_int_max = (2 ** n_bits) - 1\n",
    "        \n",
    "        # frac_bits = 7 if frac_bits >= 8 else frac_bits - 1\n",
    "        frac_bits -= 1\n",
    "    else:\n",
    "        range_int_min = 0\n",
    "        range_int_max = (2 ** n_bits)\n",
    "    # Quantization the input\n",
    "    \n",
    "    x_int = torch.round(x * (2 ** (frac_bits))).to(torch.int8)\n",
    "    x_float = torch.clamp(x_int * (1/(2 ** (frac_bits))), range_int_min, range_int_max)\n",
    "    # quant_error = torch.mean((x - x_float) ** 2)\n",
    "    frac_bits = frac_bits if isinstance(frac_bits, int) else frac_bits.item()\n",
    "    return x_int, frac_bits\n",
    "class HookRecorder:\n",
    "    def __init__(self):\n",
    "        self.recorder = dict() # Get intermediate tensor from the recorder\n",
    "        self.handlers = list()\n",
    "    \n",
    "    def _register_hooker(self, name):\n",
    "        self.recorder[name] = dict()\n",
    "        def named_hooker(module, input: Tuple[torch.Tensor], output: torch.Tensor):\n",
    "            # input_repr = input[0].int_repr().detach().cpu().numpy()\n",
    "            input = input[0].dequantize().detach().cpu().numpy()\n",
    "            input = Fxp(input, signed=True, n_word=8, overflow='saturate')\n",
    "            x_frac = input.n_frac\n",
    "            x_int = torch.tensor(np.array(input << input.n_frac).astype(np.int8))\n",
    "            # x_int, x_frac = qfmt_quanize(input[0], 8, True)\n",
    "            \n",
    "            self.recorder[name][\"input\"] = x_int\n",
    "            self.recorder[name][\"i_f\"] = x_frac\n",
    "            self.recorder[name]['input_shape'] = x_int.shape\n",
    "            \n",
    "            # output_repr = output.int_repr().detach().cpu().numpy()\n",
    "            output = output.dequantize().detach().cpu().numpy()\n",
    "            output = Fxp(output, signed=True, n_word=8, overflow='saturate')\n",
    "            y_frac = output.n_frac\n",
    "            y_int = torch.tensor(np.array(output << output.n_frac).astype(np.int8))\n",
    "            # assert torch.count_nonzero(y_int - output_repr)\n",
    "            # y_int, y_frac = qfmt_quanize(output, 8, True)\n",
    "            self.recorder[name][\"output\"] = y_int\n",
    "            self.recorder[name][\"o_f\"] = y_frac\n",
    "            self.recorder[name]['output_shape'] = y_int.shape\n",
    "            \n",
    "            \n",
    "        return named_hooker\n",
    "    \n",
    "    def register_hookers(self, target_sub_modules, layer_names):\n",
    "        for i in range(len(layer_names)):\n",
    "            module = target_sub_modules[i]\n",
    "            layer_name = layer_names[i]\n",
    "            handler = module.register_forward_hook(self._register_hooker(layer_name))\n",
    "        self.handlers.append(handler)\n",
    "        \n",
    "    def remove_handlers(self):\n",
    "        for i in self.handlers:\n",
    "            i.remove()\n",
    "        self.handlers.clear()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.remove_handlers()\n",
    "\n",
    "hook = HookRecorder()\n",
    "hook.register_hookers([qmodel.quant, qmodel.STAGE0_CONV, qmodel.STAGE1_0_CONV, \n",
    "                       qmodel.STAGE2_0_CONV, qmodel.STAGE3_0_CONV, \n",
    "                       qmodel.STAGE4_0_CONV, qmodel.LINEAR], \n",
    "                      [\"quant\", \"STAGE0_CONV\", \"STAGE1_0_CONV\", \n",
    "                       \"STAGE2_0_CONV\", \"STAGE3_0_CONV\",\n",
    "                       \"STAGE4_0_CONV\", \"LINEAR\"])\n",
    "\n",
    "qmodel(img.unsqueeze(0).to(device))\n",
    "# print(hook.recorder)\n",
    "# hook.remove_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['quant', 'STAGE0_CONV', 'STAGE1_0_CONV', 'STAGE2_0_CONV', 'STAGE3_0_CONV', 'STAGE4_0_CONV', 'LINEAR'])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "6\n",
      "1\n",
      "dict_keys(['input', 'i_f', 'input_shape', 'output', 'o_f', 'output_shape'])\n"
     ]
    }
   ],
   "source": [
    "print(hook.recorder.keys())\n",
    "print(hook.recorder['STAGE0_CONV']['input_shape'])\n",
    "print(hook.recorder['STAGE0_CONV']['output_shape'])\n",
    "print(hook.recorder['STAGE0_CONV']['i_f'])\n",
    "print(hook.recorder['STAGE0_CONV']['o_f'])\n",
    "print(hook.recorder['STAGE0_CONV'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 92, -47, -15],\n",
      "        [-13,   9,  12],\n",
      "        [-13, -23,  -2]], device='cuda:0', dtype=torch.int8)\n",
      "[[ 92. -47. -15.]\n",
      " [-13.   9.  12.]\n",
      " [-13. -23.  -2.]]\n",
      "[-15.  -5.  -4.  -1.   7.  18.  18.  -3. -78.  27.  -6. -13. -39.  -2.\n",
      "  52.   0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fxpmath import Fxp\n",
    "\n",
    "def simulate(img, qmodel, verbose=False):\n",
    "    quant = qmodel.quant.scale.item()\n",
    "    inp = torch.round(img*(1/quant)).int()\n",
    "    \n",
    "    \"\"\"\n",
    "    (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
    "    (STAGE0_CONV): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
    "    (STAGE1_0_CONV): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
    "    (STAGE2_0_CONV): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
    "    (STAGE3_0_CONV): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
    "    (STAGE4_0_CONV): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.0625, zero_point=0, padding=(1, 1))\n",
    "    (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
    "    (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
    "    (LINEAR): QuantizedLinear(in_features=128, out_features=10, scale=0.03125, zero_point=0, qscheme=torch.per_tensor_affine)\n",
    "    (dequant): DeQuantize()\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(inp)\n",
    "    qmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        qmodel = qmodel.to(device)\n",
    "        \n",
    "        w = qmodel.STAGE0_CONV.weight().int_repr()[:3, 0, 0, :]\n",
    "        fw = qmodel.STAGE0_CONV.weight().dequantize()[:3, 0, 0, :].detach().cpu().numpy()\n",
    "        fb = qmodel.STAGE0_CONV.bias().dequantize().detach().cpu().numpy()\n",
    "        print(w)\n",
    "        # fw = fw*(1/scale)\n",
    "        # print(fw)\n",
    "        fw = Fxp(fw, signed=True, n_word=8)\n",
    "        print(fw<<fw.n_frac)\n",
    "        fb = Fxp(fb, signed=True, n_word=8)\n",
    "        print(fb<<fb.n_frac)\n",
    "        # scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "        \n",
    "    #     # print(model.STAGE0_CONV.weight().dequantize()[:, :3, :,:])\n",
    "    #     print(w, fw.raw())\n",
    "    #     # print(model.STAGE0_CONV.bias().dequantize()*(1/scale))\n",
    "    # return pow2s\n",
    "        \n",
    "qmodel  \n",
    "pow2s = simulate(img, qmodel)\n",
    "# print(pow2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.qconfig.activation().calculate_qparams()\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extra_preprocess(x:torch.Tensor):\n",
    "    # hint: you need to convert the original fp32 input of range (0, 1)\n",
    "    #  into int8 format of range (-128, 127)\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    from fxpmath import Fxp\n",
    "    import numpy as np\n",
    "    np_x = x.numpy()\n",
    "    x = torch.tensor(np.array(Fxp(np_x, signed=True, n_word=8).raw(), dtype=np.int8))\n",
    "    return x.to(torch.int8)\n",
    "    ############### YOUR CODE ENDS HERE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model = MCU_VGGRepC1(num_classes=10, quant=False)\n",
    "qmodel = qmodel.cpu()\n",
    "plain_model = plain_model.cpu()\n",
    "quantized_state_dict = qmodel.state_dict()\n",
    "state_dict = plain_model.state_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = qmodel.cuda()\n",
    "qmodel.STAGE0_CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.STAGE0_CONV.weight().int_repr()[:, :3, :, :]\n",
    "print(qmodel.LINEAR.weight().detach().cpu())\n",
    "w = qmodel.LINEAR.weight().detach().cpu()\n",
    "(1/qmodel.LINEAR.scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.nn.quantized.modules.conv import Conv2d\n",
    "from torch.ao.nn.quantized.modules.linear import Linear\n",
    "qmodel = qmodel.to(device)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def ddict():\n",
    "    return defaultdict(ddict)\n",
    "GRAPH = ddict()\n",
    "for name,  modules in qmodel.named_modules():\n",
    "    print(name, type(modules))\n",
    "    if isinstance(modules, Conv2d):\n",
    "        # print(f'{name} weight: {modules.weight()}')\n",
    "        # print(modules.weight().element_size())\n",
    "        # print(modules.scale, modules.zero_point)\n",
    "        # print(modules.weight().int_repr())\n",
    "        GRAPH[name]['scale'] = modules.scale\n",
    "        GRAPH[name]['zero_point'] = modules.zero_point\n",
    "        GRAPH[name]['weight']['float'] = model.state_dict()[f'{name}.weight'].detach().cpu().numpy()\n",
    "        GRAPH[name]['weight']['int'] = modules.weight().detach().cpu()#.int_repr()\n",
    "        \n",
    "        if modules.bias is not None:\n",
    "            GRAPH[name]['bias']['float'] = model.state_dict()[f'{name}.bias'].detach().cpu().numpy()\n",
    "            GRAPH[name]['bias']['qfloat'] = modules.bias().detach().cpu().numpy()\n",
    "            # GRAPH[name]['bias']['int'] = modules.bias().detach().cpu().int_repr().numpy()\n",
    "            GRAPH[name]['bias_scale'] = modules.scale\n",
    "            GRAPH[name]['bias_zero_point'] = modules.zero_point\n",
    "            # print(f'{name} bias: {modules.bias()}')\n",
    "            \n",
    "    elif isinstance(modules, Linear):\n",
    "        # print(modules.weight().element_size())\n",
    "        GRAPH[name]['weight']['int'] = modules.weight().detach().cpu()#.int_repr()\n",
    "        GRAPH[name]['weight']['float'] = model.state_dict()[f'{name}.weight'].detach().cpu().numpy()\n",
    "        GRAPH[name]['scale'] = modules.scale\n",
    "        GRAPH[name]['zero_point'] = modules.zero_point\n",
    "        # print(f'{name} weight: {modules.weight().int_repr()}')\n",
    "        if modules.bias is not None:\n",
    "            GRAPH[name]['bias']['float'] = model.state_dict()[f'{name}.bias'].detach().cpu().numpy()\n",
    "            GRAPH[name]['bias']['qfloat'] = modules.bias().detach().cpu().numpy()\n",
    "            GRAPH[name]['bias_scale'] = modules.scale\n",
    "            GRAPH[name]['bias_zero_point'] = modules.zero_point\n",
    "        #     print(f'{name} bias: {modules.bias()}')\n",
    "# print(qmodel)\n",
    "# print(get_accuracy(qmodel, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "# qweight = GRAPH['STAGE0_CONV']['weight']['int'].dequantize()[:,:3,:,:].numpy().reshape(-1)\n",
    "qweight = GRAPH['STAGE0_CONV']['weight']['int'].int_repr()[:,:3,:,:].numpy().reshape(-1)\n",
    "# print(qweight.shape)\n",
    "from fxpmath import Fxp\n",
    "weight = GRAPH['STAGE0_CONV']['weight']['float'].reshape(-1)\n",
    "# weight = np.array(Fxp(weight, signed=True, n_word=8, overflow='saturate').raw())\n",
    "\n",
    "# # print(weight.n_frac)\n",
    "scale = GRAPH['STAGE0_CONV']['scale']\n",
    "zero = GRAPH['STAGE0_CONV']['zero_point']\n",
    "print(scale, zero)\n",
    "print(weight.min(), weight.max())\n",
    "weight = (qweight)*(2**scale)\n",
    "# qweight = (qweight*scale) + zero\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "# plt.hist(qweight, bins=100, lpha=0.5, label=f'Scale:{scale}, Zero:{zero}')\n",
    "# plt.hist(weight, bins=100, color='b', alpha=0.5, label=f'Original')\n",
    "# plt.hist(qweight, bins=128, color='red')\n",
    "# plt.hist(weight, bins=128, alpha=0.5, color='blue')\n",
    "plt.scatter(np.arange(len(qweight)), qweight, c='r', label=f'Scale:{scale}, Zero:{zero}')\n",
    "plt.scatter(np.arange(len(weight)), weight, c='b', label=f'Original')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# pprint(GRAPH['LINEAR'])\n",
    "# test_qint = (GRAPH['STAGE0_CONV']['weight']['int'] - GRAPH['STAGE0_CONV']['zero_point'])*(GRAPH['STAGE0_CONV']['scale'])\n",
    "# print(test_qint)\n",
    "# test_qint2 = (GRAPH['STAGE0_CONV']['weight']['float'])\n",
    "# test_qint[:,:3, :,:] - test_qint2\n",
    "# print(GRAPH['STAGE0_CONV']['bias']['float'], GRAPH['STAGE0_CONV']['bias']['qfloat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408.43 KB\n",
      "109.71 KB\n"
     ]
    }
   ],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f KB\" %(os.path.getsize(\"tmp.pt\")/1e3))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.nn.quantized.modules.conv import Conv2d as QConv2d\n",
    "from torch.ao.nn.quantized.modules.linear import Linear as QLinear\n",
    "from torch import nn\n",
    "def hook_save_params(module, input, output):\n",
    "    setattr(module, \"input_shape\", input[0].shape)\n",
    "    setattr(module, \"output_shape\", output[0].shape)\n",
    "    setattr(module, \"input\", input[0][0])\n",
    "    setattr(module, \"output\", output[0])\n",
    "\n",
    "\n",
    "def register_hooks(model:nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (QConv2d, QLinear, nn.Conv2d, nn.Linear, nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d)):\n",
    "            module.register_forward_hook(hook_save_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_hooks(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = qmodel.to(device)\n",
    "with torch.inference_mode():\n",
    "  for img, label in testloader:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    if cnt > 10:\n",
    "        break\n",
    "    qmodel(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "q_param = OrderedDict()\n",
    "q_model = copy.deepcopy(model).cpu()\n",
    "for name, modules in model.named_modules():\n",
    "    fxp_ref = Fxp(None, signed=True, n_word=8, overflow='saturate')\n",
    "    fxp_ref.config.dtype_notation = 'Q'\n",
    "    fxp_ref.config.op_method = 'repr'\n",
    "    fxp_ref.config.op_out = Fxp(None, True, n_word=8, overflow='saturate')\n",
    "    fxp_ref.config.array_output_type = 'array'\n",
    "    if isinstance(modules, nn.Conv2d):\n",
    "        weight = modules.weight.detach().cpu().numpy()\n",
    "        q_weight = Fxp(weight, like = fxp_ref)\n",
    "        n_int = q_weight.n_int\n",
    "        n_frac = q_weight.n_frac\n",
    "        print(f'{name} n_frac: {n_frac}')\n",
    "        q_weight = q_weight << n_frac # Interger convert\n",
    "        q_weight = q_weight >> n_frac # Fixed point convert\n",
    "        \n",
    "        # q_weight = q_weight.ravel()\n",
    "        # weight = weight.ravel()\n",
    "        # plt.figure(figsize=(18,6))\n",
    "        # plt.scatter(np.arange(len(q_weight)), q_weight, c='r', label=f'Q{n_int}.{n_frac}')\n",
    "        # plt.scatter(np.arange(len(weight)), weight, c='b', label=f'Original')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        print((q_weight - weight).sum())\n",
    "        # break\n",
    "        q_model.state_dict()[f'{name}.weight'].copy_(torch.Tensor((np.array(q_weight))))\n",
    "        # print(q_model.state_dict()[f'{name}.weight'])\n",
    "        q_param[f'{name}.weight'] = (n_int, n_frac)\n",
    "        # print(torch.IntTensor(np.array(q_weight)))\n",
    "        \n",
    "        # print(q_model.state_dict()[f'{name}.weight'])\n",
    "        # model[name].weight = torch.Tensor(np.array(q_weight))\n",
    "        print(f'{name}.weight error: {(q_weight - weight).sum()}')\n",
    "        if modules.bias is not None:\n",
    "            bias = modules.bias.detach().cpu().numpy()\n",
    "            q_bias = Fxp(bias, like = fxp_ref)\n",
    "            n_int = q_bias.n_int\n",
    "            n_frac = q_bias.n_frac\n",
    "            q_bias = q_bias << n_frac\n",
    "            q_bias = q_bias >> n_frac\n",
    "            \n",
    "            q_model.state_dict()[f'{name}.bias'].copy_(torch.Tensor(np.array(q_bias)))\n",
    "            q_param[f'{name}.bias'] = (n_frac)\n",
    "            print(f'{name}.bias error: {(q_bias - bias).sum()}')\n",
    "            \n",
    "        \n",
    "    elif isinstance(modules, nn.Linear):\n",
    "        weight = modules.weight.detach().cpu().numpy()\n",
    "        q_weight = Fxp(weight, like = fxp_ref)\n",
    "        n_int = q_weight.n_int\n",
    "        n_frac = q_weight.n_frac\n",
    "        q_weight = q_weight << n_frac\n",
    "        q_weight = q_weight >> n_frac\n",
    "        \n",
    "        q_model.state_dict()[f'{name}.weight'].copy_(torch.Tensor(np.array(q_weight)))\n",
    "        q_param[f'{name}.weight'] = (n_frac)\n",
    "        print(f'{name}.weight error: {(q_weight - weight).sum()}')\n",
    "        if modules.bias is not None:\n",
    "            bias = modules.bias.detach().cpu().numpy()\n",
    "            \n",
    "            q_bias = Fxp(bias, like = fxp_ref)\n",
    "            n_int = q_bias.n_int\n",
    "            n_frac = q_bias.n_frac\n",
    "            \n",
    "            \n",
    "            q_bias = q_bias << n_frac\n",
    "            q_bias = q_bias >> n_frac\n",
    "            q_model.state_dict()[f'{name}.bias'].copy_(torch.Tensor(np.array(q_bias)))\n",
    "            q_param[f'{name}.bias'] = (n_frac)\n",
    "            print(f'{name}.bias error: {(q_bias - bias).sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fxpmath import Fxp\n",
    "for name, module in qmodel.named_modules():\n",
    "    if isinstance(module, (QConv2d, QLinear, nn.Conv2d, nn.Linear)):\n",
    "        print(module.scale)\n",
    "        fxp_scale = Fxp(module.scale, signed=True, n_word=8, overflow='saturate')\n",
    "        print(f'{name} scale: {fxp_scale.n_frac}, INT8={fxp_scale<<fxp_scale.n_frac}')\n",
    "        print(name, module.input_shape, module.output_shape, module.input.shape, module.output.shape)\n",
    "    if isinstance(module, (nn.AdaptiveAvgPool2d, nn.AvgPool2d, nn.MaxPool2d)):\n",
    "        print(name, module.input_shape, module.output_shape, module.input.shape, module.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.STAGE0_CONV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
