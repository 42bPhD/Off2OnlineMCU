{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torchsummary import summary\n",
    "import os\n",
    "from utils.dataloaders import get_dataloader, get_subnet_dataloader\n",
    "from utils.train_eval import evaluate\n",
    "from utils.functions import reconstruction_model\n",
    "from utils.io import load_weights\n",
    "\n",
    "from utils.train_eval import get_accuracy\n",
    "from utils.utils import count_net_flops\n",
    "\n",
    "from bn_fold import bn_fold\n",
    "from torch import nn\n",
    "from fxpmath import Fxp\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ckpt = \"./weights/mcu_vggrepc1_vww.pth\"\n",
    "model_ckpt = \"./weights/mcu_vggrepopt_cifar10.pth\"\n",
    "data_dir = \"E:/1_TinyML/tiny/benchmark/training/visual_wake_words/vw_coco2014_96\"\n",
    "image_size = 32\n",
    "workers = 4\n",
    "batch_size = 50\n",
    "from models.model_q import MCU_VGGRep, MCU_VGGRepC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCU_VGGRepC1(\n",
      "  (quant): QuantStub()\n",
      "  (STAGE0_CONV): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE0_RELU): ReLU()\n",
      "  (STAGE1_0_CONV): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE1_0_RELU): ReLU()\n",
      "  (STAGE2_0_CONV): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE2_0_RELU): ReLU()\n",
      "  (STAGE3_0_CONV): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE3_0_RELU): ReLU()\n",
      "  (STAGE4_0_CONV): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE4_0_RELU): ReLU()\n",
      "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
      "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
      "  (LINEAR): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MCU_VGGRepC1(num_classes=10)\n",
    "model = load_weights(model, model_ckpt)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading cifar10 data...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flatten_model = reconstruction_model(model, device)\n",
    "def get_cifar10_loader():\n",
    "    print('=> loading cifar10 data...')\n",
    "    normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='E:/2_Quantization/torch2cmsis/examples/cifar/data/data_cifar10',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ]))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='E:/2_Quantization/torch2cmsis/examples/cifar/data/data_cifar10',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ]))\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_cifar10_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating Point Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before accuracy: 75.94% MAC+BN=480,640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "\n",
    "print(f\"Before accuracy: {get_accuracy(model.to(device), testloader):.2f}%\",\n",
    "        f\"MAC+BN={count_net_flops(model, (1, 3, image_size, image_size), False):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cofing Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import get_default_qconfig, quantize_dynamic, QConfig, HistogramObserver, prepare, fuse_modules\n",
    "# Custom quantization configuration 정의\n",
    "import math\n",
    "from torch.quantization import MinMaxObserver\n",
    "import torch\n",
    "\n",
    "\n",
    "class PowerOfTwoActivationObserver(HistogramObserver):\n",
    "    def __init__(self, bits=8, *args, **kwargs):\n",
    "        super(PowerOfTwoActivationObserver, self).__init__(*args, **kwargs)\n",
    "        self.bits = bits\n",
    "        self.quant_min = 0\n",
    "        self.quant_max = 2 ** bits - 1\n",
    "        self.dtype = torch.qint8  # 추가: dtype을 torch.quint8로 설정\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0))\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        min_val, max_val = self.min_val, self.max_val\n",
    "        if min_val == max_val:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            max_val = max(abs(min_val), abs(max_val))\n",
    "            scale = max_val / ((2 ** self.bits) - 1)\n",
    "            scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "            zero_point = 0\n",
    "\n",
    "        self.scale.copy_(scale_pow2)\n",
    "        self.zero_point.copy_(zero_point)\n",
    "        return self.scale, self.zero_point\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.min_val = min(self.min_val, x.min())\n",
    "        self.max_val = max(self.max_val, x.max())\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"min_val={}, max_val={}, scale={}, zero_point={}, bits={}\".format(\n",
    "            self.min_val, self.max_val, self.scale, self.zero_point, self.bits\n",
    "        )\n",
    "class PowerOfTwoWeightObserver(HistogramObserver):\n",
    "    def __init__(self, bits=8, *args, **kwargs):\n",
    "        super(PowerOfTwoWeightObserver, self).__init__(*args, **kwargs)\n",
    "        self.bits = bits\n",
    "        self.dtype = torch.qint8  # 추가: dtype을 torch.quint8로 설정\n",
    "\n",
    "        self.quant_min = -2 ** (bits - 1)\n",
    "        self.quant_max = 2 ** (bits - 1) - 1\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0))\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        min_val, max_val = self.min_val, self.max_val\n",
    "        if min_val == max_val:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            max_val = max(abs(min_val), abs(max_val))\n",
    "            scale = max_val / (2 ** (self.bits - 1) - 1)\n",
    "            scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "            zero_point = 0\n",
    "\n",
    "        self.scale.copy_(scale_pow2)\n",
    "        self.zero_point.copy_(zero_point)\n",
    "        return self.scale, self.zero_point\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.min_val = min(self.min_val, x.min())\n",
    "        self.max_val = max(self.max_val, x.max())\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"min_val={}, max_val={}, scale={}, zero_point={}, bits={}\".format(\n",
    "            self.min_val, self.max_val, self.scale, self.zero_point, self.bits\n",
    "        )\n",
    "\n",
    "backend = \"x86\"\n",
    "model.qconfig = QConfig(\n",
    "            activation=PowerOfTwoWeightObserver.with_args(bits=8, \n",
    "                                                          qscheme=torch.per_tensor_symmetric,\n",
    "                                                          dtype=torch.qint8),\n",
    "            weight=PowerOfTwoWeightObserver.with_args(bits=8,\n",
    "                                              qscheme=torch.per_tensor_symmetric,\n",
    "                                              dtype=torch.qint8)\n",
    "            )\n",
    "fuse_modules(model, [['STAGE0_CONV', 'STAGE0_RELU'],\n",
    "                     ['STAGE1_0_CONV', 'STAGE1_0_RELU'],\n",
    "                     ['STAGE2_0_CONV', 'STAGE2_0_RELU'],\n",
    "                     ['STAGE3_0_CONV', 'STAGE3_0_RELU'],\n",
    "                     ['STAGE4_0_CONV', 'STAGE4_0_RELU']], inplace=True)\n",
    "# model.qconfig = torch.quantization.QConfig(\n",
    "#     activation=HistogramObserver.with_args(dtype=torch.qint8, \n",
    "#                                            qscheme=torch.per_tensor_symmetric),\n",
    "#     weight=HistogramObserver.with_args(dtype=torch.qint8,\n",
    "#                                         qscheme=torch.per_tensor_symmetric)\n",
    "# )\n",
    "qmodel = prepare(model, inplace=False)\n",
    "# model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "# torch.backends.quantized.engine = backend\n",
    "# model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "# model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n",
    "# model.qconfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCU_VGGRepC1(\n",
       "  (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
       "  (STAGE0_CONV): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE0_RELU): Identity()\n",
       "  (STAGE1_0_CONV): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE1_0_RELU): Identity()\n",
       "  (STAGE2_0_CONV): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE2_0_RELU): Identity()\n",
       "  (STAGE3_0_CONV): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.25, zero_point=0, padding=(1, 1))\n",
       "  (STAGE3_0_RELU): Identity()\n",
       "  (STAGE4_0_CONV): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.03125, zero_point=0, padding=(1, 1))\n",
       "  (STAGE4_0_RELU): Identity()\n",
       "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
       "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
       "  (LINEAR): QuantizedLinear(in_features=128, out_features=10, scale=0.03125, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "qmodel = qmodel.to(device)\n",
    "with torch.inference_mode():\n",
    "    for img, label in trainloader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        if cnt > 10:\n",
    "            break\n",
    "        qmodel(img)\n",
    "    \n",
    "qmodel = torch.quantization.convert(qmodel, inplace=True)\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(qmodel, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "imgs = iter(testloader).__next__()[0]\n",
    "print(imgs.shape)\n",
    "_ = qmodel(imgs.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0625, -0.9688, -0.0625,  1.9375, -0.9688,  1.2188,  0.1250, -1.5000,\n",
       "         -0.9062, -0.6875]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = imgs[0]\n",
    "img.shape\n",
    "qmodel(img.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quant': {'input': tensor([[[[ 16,  17,  20,  ...,   5,   0,  -4],\n",
      "          [ 13,  13,  17,  ...,   5,   0,  -3],\n",
      "          [ 13,  13,  16,  ...,   7,   2,  -2],\n",
      "          ...,\n",
      "          [-29, -42, -47,  ..., -44, -57, -43],\n",
      "          [-32, -38, -45,  ..., -50, -48, -53],\n",
      "          [-36, -35, -40,  ..., -51, -46, -52]],\n",
      "\n",
      "         [[ -5,  -6,  -3,  ..., -14, -16, -19],\n",
      "          [ -5,  -6,  -4,  ..., -14, -16, -18],\n",
      "          [ -6,  -7,  -6,  ..., -12, -14, -17],\n",
      "          ...,\n",
      "          [  0, -11, -18,  ..., -13, -30, -19],\n",
      "          [ -3, -10, -19,  ..., -21, -21, -30],\n",
      "          [ -8,  -9, -17,  ..., -23, -20, -28]],\n",
      "\n",
      "         [[-31, -32, -30,  ..., -37, -37, -38],\n",
      "          [-30, -35, -33,  ..., -39, -39, -38],\n",
      "          [-32, -38, -37,  ..., -38, -38, -38],\n",
      "          ...,\n",
      "          [ 30,  16,  11,  ...,  15,  -2,   6],\n",
      "          [ 25,  16,   8,  ...,   7,   5,  -3],\n",
      "          [ 22,  16,   8,  ...,   4,   7,  -1]]]], dtype=torch.int8), 'i_f': 5, 'input_shape': torch.Size([1, 3, 32, 32]), 'output': tensor([[[[  33,   34,   40,  ...,   12,    1,   -9],\n",
      "          [  27,   26,   34,  ...,   11,    0,   -6],\n",
      "          [  26,   26,   33,  ...,   14,    5,   -5],\n",
      "          ...,\n",
      "          [ -58,  -85,  -96,  ...,  -89, -114,  -87],\n",
      "          [ -65,  -77,  -92,  ..., -101,  -98, -107],\n",
      "          [ -72,  -70,  -81,  ..., -103,  -93, -106]],\n",
      "\n",
      "         [[ -11,  -12,   -7,  ...,  -29,  -33,  -39],\n",
      "          [ -11,  -13,   -9,  ...,  -29,  -33,  -36],\n",
      "          [ -13,  -14,  -12,  ...,  -26,  -29,  -35],\n",
      "          ...,\n",
      "          [   1,  -24,  -36,  ...,  -27,  -61,  -39],\n",
      "          [  -7,  -22,  -39,  ...,  -42,  -42,  -61],\n",
      "          [ -16,  -18,  -35,  ...,  -47,  -40,  -58]],\n",
      "\n",
      "         [[ -62,  -64,  -60,  ...,  -75,  -75,  -78],\n",
      "          [ -60,  -71,  -66,  ...,  -79,  -79,  -77],\n",
      "          [ -64,  -78,  -75,  ...,  -77,  -77,  -78],\n",
      "          ...,\n",
      "          [  60,   33,   22,  ...,   31,   -6,   12],\n",
      "          [  52,   33,   17,  ...,   15,   12,   -7],\n",
      "          [  44,   34,   17,  ...,   10,   14,   -4]]]], dtype=torch.int8), 'o_f': 6, 'output_shape': torch.Size([1, 3, 32, 32])}, 'STAGE0_CONV': {'input': tensor([[[[  33,   34,   40,  ...,   12,    1,   -9],\n",
      "          [  27,   26,   34,  ...,   11,    0,   -6],\n",
      "          [  26,   26,   33,  ...,   14,    5,   -5],\n",
      "          ...,\n",
      "          [ -58,  -85,  -96,  ...,  -89, -114,  -87],\n",
      "          [ -65,  -77,  -92,  ..., -101,  -98, -107],\n",
      "          [ -72,  -70,  -81,  ..., -103,  -93, -106]],\n",
      "\n",
      "         [[ -11,  -12,   -7,  ...,  -29,  -33,  -39],\n",
      "          [ -11,  -13,   -9,  ...,  -29,  -33,  -36],\n",
      "          [ -13,  -14,  -12,  ...,  -26,  -29,  -35],\n",
      "          ...,\n",
      "          [   1,  -24,  -36,  ...,  -27,  -61,  -39],\n",
      "          [  -7,  -22,  -39,  ...,  -42,  -42,  -61],\n",
      "          [ -16,  -18,  -35,  ...,  -47,  -40,  -58]],\n",
      "\n",
      "         [[ -62,  -64,  -60,  ...,  -75,  -75,  -78],\n",
      "          [ -60,  -71,  -66,  ...,  -79,  -79,  -77],\n",
      "          [ -64,  -78,  -75,  ...,  -77,  -77,  -78],\n",
      "          ...,\n",
      "          [  60,   33,   22,  ...,   31,   -6,   12],\n",
      "          [  52,   33,   17,  ...,   15,   12,   -7],\n",
      "          [  44,   34,   17,  ...,   10,   14,   -4]]]], dtype=torch.int8), 'i_f': 6, 'input_shape': torch.Size([1, 3, 32, 32]), 'output': tensor([[[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  2,  ...,  0,  1,  0],\n",
      "          [ 8,  0,  0,  ...,  0,  2,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0, 25,  0],\n",
      "          [ 2,  0,  0,  ..., 25,  0,  7],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  4]],\n",
      "\n",
      "         [[12,  7,  9,  ..., 10, 11, 10],\n",
      "          [12,  7,  6,  ...,  8,  8,  6],\n",
      "          [10,  6,  7,  ..., 12,  7,  8],\n",
      "          ...,\n",
      "          [ 0,  0,  1,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  1,  ...,  0,  0,  2],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  1]],\n",
      "\n",
      "         [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  4,  0,  0],\n",
      "          [ 0,  6,  0,  ...,  4,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0, 13],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ..., 10,  1,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5,  0,  2,  ...,  0,  0,  0],\n",
      "          [ 7,  0,  3,  ...,  0,  0,  1],\n",
      "          [ 9,  0,  0,  ..., 11,  0,  3],\n",
      "          ...,\n",
      "          [ 6,  9,  0,  ...,  0,  0,  0],\n",
      "          [ 6,  4,  2,  ...,  0,  3, 22],\n",
      "          [ 2, 12,  0,  ...,  0, 11,  0]],\n",
      "\n",
      "         [[10, 10, 10,  ...,  8,  7,  6],\n",
      "          [10,  9, 10,  ...,  6,  7,  6],\n",
      "          [10,  9,  9,  ...,  3,  6,  7],\n",
      "          ...,\n",
      "          [10,  8, 11,  ...,  9, 13, 12],\n",
      "          [11,  9,  8,  ..., 13, 11,  7],\n",
      "          [11,  9,  8,  ..., 17, 10,  6]],\n",
      "\n",
      "         [[ 0,  4,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  5,  0,  ...,  2,  0,  0],\n",
      "          [ 0,  1,  0,  ...,  0,  9,  0],\n",
      "          ...,\n",
      "          [ 0,  0, 13,  ...,  0, 12,  0],\n",
      "          [ 0,  0,  0,  ..., 14,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  5,  0,  0]]]], dtype=torch.int8), 'o_f': 1, 'output_shape': torch.Size([1, 16, 16, 16])}, 'STAGE1_0_CONV': {'input': tensor([[[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  2,  ...,  0,  1,  0],\n",
      "          [ 8,  0,  0,  ...,  0,  2,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0, 25,  0],\n",
      "          [ 2,  0,  0,  ..., 25,  0,  7],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  4]],\n",
      "\n",
      "         [[12,  7,  9,  ..., 10, 11, 10],\n",
      "          [12,  7,  6,  ...,  8,  8,  6],\n",
      "          [10,  6,  7,  ..., 12,  7,  8],\n",
      "          ...,\n",
      "          [ 0,  0,  1,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  1,  ...,  0,  0,  2],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  1]],\n",
      "\n",
      "         [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  4,  0,  0],\n",
      "          [ 0,  6,  0,  ...,  4,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0, 13],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ..., 10,  1,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5,  0,  2,  ...,  0,  0,  0],\n",
      "          [ 7,  0,  3,  ...,  0,  0,  1],\n",
      "          [ 9,  0,  0,  ..., 11,  0,  3],\n",
      "          ...,\n",
      "          [ 6,  9,  0,  ...,  0,  0,  0],\n",
      "          [ 6,  4,  2,  ...,  0,  3, 22],\n",
      "          [ 2, 12,  0,  ...,  0, 11,  0]],\n",
      "\n",
      "         [[10, 10, 10,  ...,  8,  7,  6],\n",
      "          [10,  9, 10,  ...,  6,  7,  6],\n",
      "          [10,  9,  9,  ...,  3,  6,  7],\n",
      "          ...,\n",
      "          [10,  8, 11,  ...,  9, 13, 12],\n",
      "          [11,  9,  8,  ..., 13, 11,  7],\n",
      "          [11,  9,  8,  ..., 17, 10,  6]],\n",
      "\n",
      "         [[ 0,  4,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  5,  0,  ...,  2,  0,  0],\n",
      "          [ 0,  1,  0,  ...,  0,  9,  0],\n",
      "          ...,\n",
      "          [ 0,  0, 13,  ...,  0, 12,  0],\n",
      "          [ 0,  0,  0,  ..., 14,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  5,  0,  0]]]], dtype=torch.int8), 'i_f': 1, 'input_shape': torch.Size([1, 16, 16, 16]), 'output': tensor([[[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ..., 14,  0,  0],\n",
      "          [ 0, 18,  0,  ...,  3,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0, 13,  ...,  4, 18,  0],\n",
      "          [ 0, 14,  0,  ...,  2,  3,  6],\n",
      "          [ 0, 12,  6,  ...,  4,  7,  7]],\n",
      "\n",
      "         [[ 9,  2,  3,  ...,  8,  9,  8],\n",
      "          [18, 18,  6,  ..., 15, 19, 24],\n",
      "          [17, 15, 15,  ..., 15, 12, 29],\n",
      "          ...,\n",
      "          [11,  6, 15,  ..., 12, 15, 12],\n",
      "          [16, 12,  0,  ..., 14, 11, 11],\n",
      "          [15, 13,  9,  ..., 14, 12, 13]],\n",
      "\n",
      "         [[20, 19, 20,  ..., 21, 19, 16],\n",
      "          [18,  0, 14,  ...,  0,  3,  0],\n",
      "          [11,  0,  0,  ...,  0,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0,  2],\n",
      "          [14,  0,  0,  ...,  8, 13, 12],\n",
      "          [33,  0,  0,  ..., 13,  7,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[16, 18, 20,  ..., 16, 16, 19],\n",
      "          [ 0, 11,  7,  ..., 10,  3, 11],\n",
      "          [ 0,  4,  0,  ...,  0,  3,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ..., 20, 18, 15],\n",
      "          [ 0,  0,  0,  ..., 20, 15, 16],\n",
      "          [ 3,  2,  8,  ..., 13, 11,  0]],\n",
      "\n",
      "         [[11, 13, 13,  ..., 12, 14, 13],\n",
      "          [11, 17, 13,  ...,  7,  9, 14],\n",
      "          [ 7,  8, 18,  ..., 14,  8, 12],\n",
      "          ...,\n",
      "          [ 3,  7,  6,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "         [[ 1, 10,  9,  ..., 10,  7, 12],\n",
      "          [ 0,  0,  7,  ...,  0, 10,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  9,  2],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ..., 14,  5,  8],\n",
      "          [ 0,  0,  0,  ...,  6,  5,  4],\n",
      "          [ 0,  9,  0,  ...,  6,  0,  2]]]], dtype=torch.int8), 'o_f': 1, 'output_shape': torch.Size([1, 16, 8, 8])}, 'STAGE2_0_CONV': {'input': tensor([[[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ..., 14,  0,  0],\n",
      "          [ 0, 18,  0,  ...,  3,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0, 13,  ...,  4, 18,  0],\n",
      "          [ 0, 14,  0,  ...,  2,  3,  6],\n",
      "          [ 0, 12,  6,  ...,  4,  7,  7]],\n",
      "\n",
      "         [[ 9,  2,  3,  ...,  8,  9,  8],\n",
      "          [18, 18,  6,  ..., 15, 19, 24],\n",
      "          [17, 15, 15,  ..., 15, 12, 29],\n",
      "          ...,\n",
      "          [11,  6, 15,  ..., 12, 15, 12],\n",
      "          [16, 12,  0,  ..., 14, 11, 11],\n",
      "          [15, 13,  9,  ..., 14, 12, 13]],\n",
      "\n",
      "         [[20, 19, 20,  ..., 21, 19, 16],\n",
      "          [18,  0, 14,  ...,  0,  3,  0],\n",
      "          [11,  0,  0,  ...,  0,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0,  2],\n",
      "          [14,  0,  0,  ...,  8, 13, 12],\n",
      "          [33,  0,  0,  ..., 13,  7,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[16, 18, 20,  ..., 16, 16, 19],\n",
      "          [ 0, 11,  7,  ..., 10,  3, 11],\n",
      "          [ 0,  4,  0,  ...,  0,  3,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ..., 20, 18, 15],\n",
      "          [ 0,  0,  0,  ..., 20, 15, 16],\n",
      "          [ 3,  2,  8,  ..., 13, 11,  0]],\n",
      "\n",
      "         [[11, 13, 13,  ..., 12, 14, 13],\n",
      "          [11, 17, 13,  ...,  7,  9, 14],\n",
      "          [ 7,  8, 18,  ..., 14,  8, 12],\n",
      "          ...,\n",
      "          [ 3,  7,  6,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "         [[ 1, 10,  9,  ..., 10,  7, 12],\n",
      "          [ 0,  0,  7,  ...,  0, 10,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  9,  2],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ..., 14,  5,  8],\n",
      "          [ 0,  0,  0,  ...,  6,  5,  4],\n",
      "          [ 0,  9,  0,  ...,  6,  0,  2]]]], dtype=torch.int8), 'i_f': 1, 'input_shape': torch.Size([1, 16, 8, 8]), 'output': tensor([[[[ 7,  3,  2, 10],\n",
      "          [ 2, 11,  9,  8],\n",
      "          [ 6, 16, 18,  7],\n",
      "          [ 0,  2,  0,  0]],\n",
      "\n",
      "         [[ 0, 12,  6,  3],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0, 10, 14]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [11,  0,  0,  1],\n",
      "          [16, 10,  8,  3],\n",
      "          [20, 21,  8,  9]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0,  0, 10, 15],\n",
      "          [ 0,  5,  9,  2],\n",
      "          [ 0,  9,  3,  0]],\n",
      "\n",
      "         [[ 0,  3, 12,  5],\n",
      "          [ 6,  8,  6,  0],\n",
      "          [ 8, 20, 17,  8],\n",
      "          [ 0,  1,  0,  0]],\n",
      "\n",
      "         [[ 2, 15,  8, 11],\n",
      "          [ 0,  2,  0,  2],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  2],\n",
      "          [ 1, 19, 13, 16],\n",
      "          [ 0, 10, 15, 12],\n",
      "          [ 5, 13,  4,  1]],\n",
      "\n",
      "         [[ 2,  0,  0,  0],\n",
      "          [19,  8, 20,  6],\n",
      "          [ 0, 12,  0,  0],\n",
      "          [ 0,  1,  5, 12]],\n",
      "\n",
      "         [[ 3,  0,  7,  1],\n",
      "          [ 1,  0,  0,  0],\n",
      "          [ 0,  0,  8, 17],\n",
      "          [ 0,  6, 11,  5]],\n",
      "\n",
      "         [[ 8, 11, 12, 12],\n",
      "          [ 2,  7,  6,  9],\n",
      "          [ 4, 12,  6,  7],\n",
      "          [16,  3,  9, 13]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  6],\n",
      "          [ 0, 14,  0,  0],\n",
      "          [11,  0,  2,  0]],\n",
      "\n",
      "         [[ 0,  5,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0, 16,  8,  6],\n",
      "          [ 1,  0,  3,  0]],\n",
      "\n",
      "         [[ 0,  0,  3,  0],\n",
      "          [ 9,  6, 17, 15],\n",
      "          [ 0,  0, 16, 26],\n",
      "          [ 0, 15, 12,  8]],\n",
      "\n",
      "         [[ 0,  0,  0,  3],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0, 10, 12, 16],\n",
      "          [ 0, 26,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [10,  2, 16, 16],\n",
      "          [ 0,  5,  2,  0],\n",
      "          [ 0,  2,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0, 14, 10, 10],\n",
      "          [ 1, 14, 13,  3],\n",
      "          [ 0,  7,  1,  1]],\n",
      "\n",
      "         [[ 1,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0, 11,  9]],\n",
      "\n",
      "         [[ 2,  0,  1,  0],\n",
      "          [24, 20,  4,  7],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 2,  5,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  3,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  1,  0],\n",
      "          [ 0,  2,  0,  0]],\n",
      "\n",
      "         [[ 2,  0,  0,  0],\n",
      "          [12, 10, 18, 16],\n",
      "          [14, 10,  1,  0],\n",
      "          [15,  8,  0,  0]],\n",
      "\n",
      "         [[17,  0,  0,  9],\n",
      "          [ 1,  0,  0,  0],\n",
      "          [ 0,  8,  0,  7],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 4,  2,  1,  3],\n",
      "          [ 0,  3,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  4,  8]],\n",
      "\n",
      "         [[ 0,  6,  8,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 5,  3, 16, 19],\n",
      "          [14, 13,  1,  0]],\n",
      "\n",
      "         [[ 6,  1,  2,  0],\n",
      "          [ 0,  3, 13,  2],\n",
      "          [20,  0,  5,  3],\n",
      "          [23, 10,  6, 14]],\n",
      "\n",
      "         [[ 7,  6,  4,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  7,  0,  0],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 2,  2,  0,  0],\n",
      "          [ 5,  1,  3,  5],\n",
      "          [ 2, 12, 17,  6],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 9,  0,  0,  6],\n",
      "          [ 0, 11, 10,  1],\n",
      "          [ 0,  0,  0,  2],\n",
      "          [ 0, 14, 14,  6]],\n",
      "\n",
      "         [[17, 34, 35, 23],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0, 12, 13]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0,  0,  1,  0],\n",
      "          [ 0,  8,  7,  1],\n",
      "          [ 6, 15, 20, 19]],\n",
      "\n",
      "         [[17, 14,  0,  7],\n",
      "          [ 0,  0,  0,  2],\n",
      "          [ 9,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 0, 11, 11,  0],\n",
      "          [ 0,  7,  8,  0],\n",
      "          [ 0,  0,  0,  6],\n",
      "          [ 8, 11,  0, 15]],\n",
      "\n",
      "         [[ 0,  4,  3,  0],\n",
      "          [12,  0,  0,  0],\n",
      "          [ 2,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0]]]], dtype=torch.int8), 'o_f': 1, 'output_shape': torch.Size([1, 32, 4, 4])}, 'STAGE3_0_CONV': {'input': tensor([[[[ 7,  3,  2, 10],\n",
      "          [ 2, 11,  9,  8],\n",
      "          [ 6, 16, 18,  7],\n",
      "          [ 0,  2,  0,  0]],\n",
      "\n",
      "         [[ 0, 12,  6,  3],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0, 10, 14]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [11,  0,  0,  1],\n",
      "          [16, 10,  8,  3],\n",
      "          [20, 21,  8,  9]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0,  0, 10, 15],\n",
      "          [ 0,  5,  9,  2],\n",
      "          [ 0,  9,  3,  0]],\n",
      "\n",
      "         [[ 0,  3, 12,  5],\n",
      "          [ 6,  8,  6,  0],\n",
      "          [ 8, 20, 17,  8],\n",
      "          [ 0,  1,  0,  0]],\n",
      "\n",
      "         [[ 2, 15,  8, 11],\n",
      "          [ 0,  2,  0,  2],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  2],\n",
      "          [ 1, 19, 13, 16],\n",
      "          [ 0, 10, 15, 12],\n",
      "          [ 5, 13,  4,  1]],\n",
      "\n",
      "         [[ 2,  0,  0,  0],\n",
      "          [19,  8, 20,  6],\n",
      "          [ 0, 12,  0,  0],\n",
      "          [ 0,  1,  5, 12]],\n",
      "\n",
      "         [[ 3,  0,  7,  1],\n",
      "          [ 1,  0,  0,  0],\n",
      "          [ 0,  0,  8, 17],\n",
      "          [ 0,  6, 11,  5]],\n",
      "\n",
      "         [[ 8, 11, 12, 12],\n",
      "          [ 2,  7,  6,  9],\n",
      "          [ 4, 12,  6,  7],\n",
      "          [16,  3,  9, 13]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  6],\n",
      "          [ 0, 14,  0,  0],\n",
      "          [11,  0,  2,  0]],\n",
      "\n",
      "         [[ 0,  5,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0, 16,  8,  6],\n",
      "          [ 1,  0,  3,  0]],\n",
      "\n",
      "         [[ 0,  0,  3,  0],\n",
      "          [ 9,  6, 17, 15],\n",
      "          [ 0,  0, 16, 26],\n",
      "          [ 0, 15, 12,  8]],\n",
      "\n",
      "         [[ 0,  0,  0,  3],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0, 10, 12, 16],\n",
      "          [ 0, 26,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [10,  2, 16, 16],\n",
      "          [ 0,  5,  2,  0],\n",
      "          [ 0,  2,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0, 14, 10, 10],\n",
      "          [ 1, 14, 13,  3],\n",
      "          [ 0,  7,  1,  1]],\n",
      "\n",
      "         [[ 1,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0, 11,  9]],\n",
      "\n",
      "         [[ 2,  0,  1,  0],\n",
      "          [24, 20,  4,  7],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 2,  5,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  3,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  1,  0],\n",
      "          [ 0,  2,  0,  0]],\n",
      "\n",
      "         [[ 2,  0,  0,  0],\n",
      "          [12, 10, 18, 16],\n",
      "          [14, 10,  1,  0],\n",
      "          [15,  8,  0,  0]],\n",
      "\n",
      "         [[17,  0,  0,  9],\n",
      "          [ 1,  0,  0,  0],\n",
      "          [ 0,  8,  0,  7],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 4,  2,  1,  3],\n",
      "          [ 0,  3,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  4,  8]],\n",
      "\n",
      "         [[ 0,  6,  8,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 5,  3, 16, 19],\n",
      "          [14, 13,  1,  0]],\n",
      "\n",
      "         [[ 6,  1,  2,  0],\n",
      "          [ 0,  3, 13,  2],\n",
      "          [20,  0,  5,  3],\n",
      "          [23, 10,  6, 14]],\n",
      "\n",
      "         [[ 7,  6,  4,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  7,  0,  0],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 2,  2,  0,  0],\n",
      "          [ 5,  1,  3,  5],\n",
      "          [ 2, 12, 17,  6],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 9,  0,  0,  6],\n",
      "          [ 0, 11, 10,  1],\n",
      "          [ 0,  0,  0,  2],\n",
      "          [ 0, 14, 14,  6]],\n",
      "\n",
      "         [[17, 34, 35, 23],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0],\n",
      "          [ 0,  0, 12, 13]],\n",
      "\n",
      "         [[ 0,  0,  0,  0],\n",
      "          [ 0,  0,  1,  0],\n",
      "          [ 0,  8,  7,  1],\n",
      "          [ 6, 15, 20, 19]],\n",
      "\n",
      "         [[17, 14,  0,  7],\n",
      "          [ 0,  0,  0,  2],\n",
      "          [ 9,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0]],\n",
      "\n",
      "         [[ 0, 11, 11,  0],\n",
      "          [ 0,  7,  8,  0],\n",
      "          [ 0,  0,  0,  6],\n",
      "          [ 8, 11,  0, 15]],\n",
      "\n",
      "         [[ 0,  4,  3,  0],\n",
      "          [12,  0,  0,  0],\n",
      "          [ 2,  0,  0,  0],\n",
      "          [ 0,  0,  0,  0]]]], dtype=torch.int8), 'i_f': 1, 'input_shape': torch.Size([1, 32, 4, 4]), 'output': tensor([[[[ 7,  0],\n",
      "          [ 2,  0]],\n",
      "\n",
      "         [[11, 16],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 2,  7]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  5]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 4,  0]],\n",
      "\n",
      "         [[ 0,  2],\n",
      "          [ 9, 23]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 10]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[10,  0],\n",
      "          [ 2,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 3,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  6],\n",
      "          [ 3,  2]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 7, 19],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 8,  3]],\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 14]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 5, 15],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 28]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[14,  4],\n",
      "          [ 9,  0]],\n",
      "\n",
      "         [[ 5,  8],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 5,  0],\n",
      "          [ 0, 11]],\n",
      "\n",
      "         [[ 0,  2],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 3, 10],\n",
      "          [10,  0]],\n",
      "\n",
      "         [[14,  6],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0, 12],\n",
      "          [ 0,  1]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 7,  0]],\n",
      "\n",
      "         [[ 9, 15],\n",
      "          [ 5,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 11]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[12,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1, 10]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  5]],\n",
      "\n",
      "         [[ 6, 14],\n",
      "          [14, 10]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [14,  0]],\n",
      "\n",
      "         [[ 0,  4],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 6, 12],\n",
      "          [ 3,  8]],\n",
      "\n",
      "         [[13,  6],\n",
      "          [10,  0]],\n",
      "\n",
      "         [[13, 21],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  3]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  4]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  7],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 16]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 5,  8]],\n",
      "\n",
      "         [[ 0,  1],\n",
      "          [ 0,  3]]]], dtype=torch.int8), 'o_f': 2, 'output_shape': torch.Size([1, 64, 2, 2])}, 'STAGE4_0_CONV': {'input': tensor([[[[ 7,  0],\n",
      "          [ 2,  0]],\n",
      "\n",
      "         [[11, 16],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 2,  7]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  5]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 4,  0]],\n",
      "\n",
      "         [[ 0,  2],\n",
      "          [ 9, 23]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 10]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[10,  0],\n",
      "          [ 2,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 3,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  6],\n",
      "          [ 3,  2]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 7, 19],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 8,  3]],\n",
      "\n",
      "         [[ 2,  2],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 14]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 5, 15],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 28]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[14,  4],\n",
      "          [ 9,  0]],\n",
      "\n",
      "         [[ 5,  8],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 5,  0],\n",
      "          [ 0, 11]],\n",
      "\n",
      "         [[ 0,  2],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 3, 10],\n",
      "          [10,  0]],\n",
      "\n",
      "         [[14,  6],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0, 12],\n",
      "          [ 0,  1]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 7,  0]],\n",
      "\n",
      "         [[ 9, 15],\n",
      "          [ 5,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 11]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[12,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 1, 10]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  5]],\n",
      "\n",
      "         [[ 6, 14],\n",
      "          [14, 10]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [14,  0]],\n",
      "\n",
      "         [[ 0,  4],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 6, 12],\n",
      "          [ 3,  8]],\n",
      "\n",
      "         [[13,  6],\n",
      "          [10,  0]],\n",
      "\n",
      "         [[13, 21],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  3]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  4]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  7],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0, 16]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 0,  0]],\n",
      "\n",
      "         [[ 0,  0],\n",
      "          [ 5,  8]],\n",
      "\n",
      "         [[ 0,  1],\n",
      "          [ 0,  3]]]], dtype=torch.int8), 'i_f': 2, 'input_shape': torch.Size([1, 64, 2, 2]), 'output': tensor([[[[19]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[56]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[33]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[16]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[52]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]],\n",
      "\n",
      "         [[ 0]]]], dtype=torch.int8), 'o_f': 5, 'output_shape': torch.Size([1, 128, 1, 1])}, 'LINEAR': {'input': tensor([[19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0, 56,  0,  0, 33,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 52,  0,  0,\n",
      "          0,  0]], dtype=torch.int8), 'i_f': 5, 'input_shape': torch.Size([1, 128]), 'output': tensor([[ -2, -31,  -2,  62, -31,  39,   4, -48, -29, -22]], dtype=torch.int8), 'o_f': 5, 'output_shape': torch.Size([1, 10])}}\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from fxpmath import Fxp\n",
    "import numpy as np\n",
    "\n",
    "def qfmt_quanize(x, n_bits=8, signed=True):\n",
    "    range_min, range_max = torch.min(x), torch.max(x)\n",
    "    range_abs = torch.max(torch.abs(range_min), torch.abs(range_max))\n",
    "    int_bits = torch.ceil(torch.log2(range_abs)).type(torch.int8)\n",
    "    frac_bits = n_bits - int_bits\n",
    "    if signed:\n",
    "        range_int_min = -(2 ** n_bits)\n",
    "        range_int_max = (2 ** n_bits) - 1\n",
    "        \n",
    "        # frac_bits = 7 if frac_bits >= 8 else frac_bits - 1\n",
    "        frac_bits -= 1\n",
    "    else:\n",
    "        range_int_min = 0\n",
    "        range_int_max = (2 ** n_bits)\n",
    "    # Quantization the input\n",
    "    \n",
    "    x_int = torch.round(x * (2 ** (frac_bits))).to(torch.int8)\n",
    "    x_float = torch.clamp(x_int * (1/(2 ** (frac_bits))), range_int_min, range_int_max)\n",
    "    # quant_error = torch.mean((x - x_float) ** 2)\n",
    "    frac_bits = frac_bits if isinstance(frac_bits, int) else frac_bits.item()\n",
    "    return x_int, frac_bits\n",
    "class HookRecorder:\n",
    "    def __init__(self):\n",
    "        self.recorder = dict() # Get intermediate tensor from the recorder\n",
    "        self.handlers = list()\n",
    "    \n",
    "    def _register_hooker(self, name):\n",
    "        self.recorder[name] = dict()\n",
    "        def named_hooker(module, input: Tuple[torch.Tensor], output: torch.Tensor):\n",
    "            input = input[0].dequantize().detach().cpu().numpy()\n",
    "            input = Fxp(input, signed=True, n_word=8, overflow='saturate')\n",
    "            x_frac = input.n_frac\n",
    "            x_int = torch.tensor(np.array(input << input.n_frac).astype(np.int8))\n",
    "            # x_int, x_frac = qfmt_quanize(input[0], 8, True)\n",
    "            \n",
    "            self.recorder[name][\"input\"] = x_int\n",
    "            self.recorder[name][\"i_f\"] = x_frac\n",
    "            self.recorder[name]['input_shape'] = x_int.shape\n",
    "            \n",
    "            output = output.dequantize().detach().cpu().numpy()\n",
    "            output = Fxp(output, signed=True, n_word=8, overflow='saturate')\n",
    "            y_frac = output.n_frac\n",
    "            y_int = torch.tensor(np.array(output << output.n_frac).astype(np.int8))\n",
    "            # y_int, y_frac = qfmt_quanize(output, 8, True)\n",
    "            self.recorder[name][\"output\"] = y_int\n",
    "            self.recorder[name][\"o_f\"] = y_frac\n",
    "            self.recorder[name]['output_shape'] = y_int.shape\n",
    "            \n",
    "            \n",
    "        return named_hooker\n",
    "    \n",
    "    def register_hookers(self, target_sub_modules, layer_names):\n",
    "        for i in range(len(layer_names)):\n",
    "            module = target_sub_modules[i]\n",
    "            layer_name = layer_names[i]\n",
    "            handler = module.register_forward_hook(self._register_hooker(layer_name))\n",
    "        self.handlers.append(handler)\n",
    "        \n",
    "    def remove_handlers(self):\n",
    "        for i in self.handlers:\n",
    "            i.remove()\n",
    "        self.handlers.clear()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.remove_handlers()\n",
    "\n",
    "hook = HookRecorder()\n",
    "hook.register_hookers([qmodel.quant, qmodel.STAGE0_CONV, qmodel.STAGE1_0_CONV, \n",
    "                       qmodel.STAGE2_0_CONV, qmodel.STAGE3_0_CONV, \n",
    "                       qmodel.STAGE4_0_CONV, qmodel.LINEAR], \n",
    "                      [\"quant\", \"STAGE0_CONV\", \"STAGE1_0_CONV\", \n",
    "                       \"STAGE2_0_CONV\", \"STAGE3_0_CONV\",\n",
    "                       \"STAGE4_0_CONV\", \"LINEAR\"])\n",
    "\n",
    "qmodel(img.unsqueeze(0).to(device))\n",
    "# print(hook.recorder)\n",
    "# hook.remove_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['quant', 'STAGE0_CONV', 'STAGE1_0_CONV', 'STAGE2_0_CONV', 'STAGE3_0_CONV', 'STAGE4_0_CONV', 'LINEAR'])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "6\n",
      "1\n",
      "dict_keys(['input', 'i_f', 'input_shape', 'output', 'o_f', 'output_shape'])\n"
     ]
    }
   ],
   "source": [
    "print(hook.recorder.keys())\n",
    "print(hook.recorder['STAGE0_CONV']['input_shape'])\n",
    "print(hook.recorder['STAGE0_CONV']['output_shape'])\n",
    "print(hook.recorder['STAGE0_CONV']['i_f'])\n",
    "print(hook.recorder['STAGE0_CONV']['o_f'])\n",
    "print(hook.recorder['STAGE0_CONV'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fxpmath import Fxp\n",
    "\n",
    "def simulate(img, qmodel, verbose=False):\n",
    "    quant = qmodel.quant.scale.item()\n",
    "    inp = torch.round(img*(1/quant)).int()\n",
    "    \n",
    "    \"\"\"\n",
    "    (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
    "    (STAGE0_CONV): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
    "    (STAGE1_0_CONV): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
    "    (STAGE2_0_CONV): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
    "    (STAGE3_0_CONV): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
    "    (STAGE4_0_CONV): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.0625, zero_point=0, padding=(1, 1))\n",
    "    (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
    "    (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
    "    (LINEAR): QuantizedLinear(in_features=128, out_features=10, scale=0.03125, zero_point=0, qscheme=torch.per_tensor_affine)\n",
    "    (dequant): DeQuantize()\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(inp)\n",
    "    qmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        qmodel = qmodel.to(device)\n",
    "        \n",
    "        w = qmodel.STAGE0_CONV.weight().int_repr()[:3, 0, 0, :]\n",
    "        fw = qmodel.STAGE0_CONV.weight().dequantize()[:3, 0, 0, :].detach().cpu().numpy()\n",
    "        print(w)\n",
    "        # fw = fw*(1/scale)\n",
    "        # print(fw)\n",
    "        fw = Fxp(fw, signed=True, n_word=8)\n",
    "        print(fw<<fw.n_frac)\n",
    "        # scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "        \n",
    "    #     # print(model.STAGE0_CONV.weight().dequantize()[:, :3, :,:])\n",
    "    #     print(w, fw.raw())\n",
    "    #     # print(model.STAGE0_CONV.bias().dequantize()*(1/scale))\n",
    "    # return pow2s\n",
    "        \n",
    "qmodel  \n",
    "pow2s = simulate(img, qmodel)\n",
    "# print(pow2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.qconfig.activation().calculate_qparams()\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extra_preprocess(x:torch.Tensor):\n",
    "    # hint: you need to convert the original fp32 input of range (0, 1)\n",
    "    #  into int8 format of range (-128, 127)\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    from fxpmath import Fxp\n",
    "    import numpy as np\n",
    "    np_x = x.numpy()\n",
    "    x = torch.tensor(np.array(Fxp(np_x, signed=True, n_word=8).raw(), dtype=np.int8))\n",
    "    return x.to(torch.int8)\n",
    "    ############### YOUR CODE ENDS HERE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model = MCU_VGGRepC1(num_classes=10, quant=False)\n",
    "qmodel = qmodel.cpu()\n",
    "plain_model = plain_model.cpu()\n",
    "quantized_state_dict = qmodel.state_dict()\n",
    "state_dict = plain_model.state_dict()\n",
    "\n",
    "def qfmt_quanize(x, n_bits=8, signed=True):\n",
    "    range_min, range_max = torch.min(x), torch.max(x)\n",
    "    range_abs = torch.max(torch.abs(range_min), torch.abs(range_max))\n",
    "    int_bits = torch.ceil(torch.log2(range_abs)).type(torch.int8)\n",
    "    frac_bits = n_bits - int_bits\n",
    "    if signed:\n",
    "        range_int_min = -(2 ** n_bits)\n",
    "        range_int_max = (2 ** n_bits) - 1\n",
    "        \n",
    "        # frac_bits = 7 if frac_bits >= 8 else frac_bits - 1\n",
    "        frac_bits -= 1\n",
    "    else:\n",
    "        range_int_min = 0\n",
    "        range_int_max = (2 ** n_bits)\n",
    "    # Quantization the input\n",
    "    \n",
    "    x_int = torch.round(x * (2 ** (frac_bits))).to(torch.int8)\n",
    "    x_float = torch.clamp(x_int * (1/(2 ** (frac_bits))), range_int_min, range_int_max)\n",
    "    # quant_error = torch.mean((x - x_float) ** 2)\n",
    "    frac_bits = frac_bits if isinstance(frac_bits, int) else frac_bits.item()\n",
    "    return x_float, frac_bits\n",
    "\n",
    "def input_process(inputs):\n",
    "    \n",
    "    return qfmt_quanize(inputs, 8, True)[0]\n",
    "    \n",
    "# 가중치와 바이어스 복사\n",
    "for name, param in state_dict.items():\n",
    "    if name in quantized_state_dict:\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            # Quantization된 모델의 가중치/바이어스 텐서 가져오기\n",
    "            quantized_param = quantized_state_dict[name]\n",
    "            \n",
    "            dequantized_param = quantized_param.dequantize()\n",
    "            # print(dequantized_param.size(), param.size())\n",
    "            # 첫 번째 차원 크기 비교\n",
    "            if dequantized_param.dim()>1 and dequantized_param.size(1) != param.size(1):\n",
    "                if dequantized_param.size(1) == param.size(1) + 1:\n",
    "                    \n",
    "                    dequantized_param = dequantized_param[:, :-1]  # 첫 번째 차원의 크기가 1 더 크면 첫 번째 채널 제거\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected size mismatch in {name}: {dequantized_param.size()} vs {param.size()}\")\n",
    "            # 크기 조정\n",
    "            if dequantized_param.size() != param.size():\n",
    "                dequantized_param = dequantized_param.view(param.size())\n",
    "            \n",
    "            # 가중치/바이어스 복사\n",
    "            param.data.copy_(dequantized_param)\n",
    "qmodel = qmodel.to(device)\n",
    "plain_model = plain_model.to(device)\n",
    "print(get_accuracy(plain_model, testloader, extra_preprocess=input_process), get_accuracy(qmodel, testloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.train_eval import AverageMeter, ProgressMeter, accuracy\n",
    "import time\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_Int_accuracy(model: nn.Module,\n",
    "                    dataloader: DataLoader,\n",
    "                    extra_preprocess = None,\n",
    "                    device:str = 'cuda:0') -> float:\n",
    "    model.eval()\n",
    "    \n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "        # Move the data from CPU to GPU\n",
    "        inputs = inputs.cpu()    \n",
    "        if extra_preprocess is not None:\n",
    "            for preprocess in extra_preprocess:\n",
    "                inputs = preprocess(inputs)\n",
    "\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "    return (num_correct / num_samples * 100).item()\n",
    "\n",
    "def evaluate(dataloader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(dataloader), [batch_time, losses, top1, top5], prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(dataloader):\n",
    "            model = model.cuda()\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(\n",
    "            top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg\n",
    "    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = qmodel.cuda()\n",
    "qmodel.STAGE0_CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qmodel.STAGE0_CONV.weight().int_repr()[:, :3, :, :]\n",
    "# print(qmodel.LINEAR.weight().detach().cpu())\n",
    "# w = qmodel.LINEAR.weight().detach().cpu()\n",
    "# (1/qmodel.LINEAR.scale)\n",
    "# print(w.data.int_repr())\n",
    "import math\n",
    "from torch.quantization import get_observer_state_dict\n",
    "# qmodel(next(iter(trainloader))[0])\n",
    "# w.dequantize()\n",
    "observer_dict = get_observer_state_dict(qmodel)\n",
    "print(observer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.nn.quantized.modules.conv import Conv2d\n",
    "from torch.ao.nn.quantized.modules.linear import Linear\n",
    "qmodel = qmodel.to(device)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def ddict():\n",
    "    return defaultdict(ddict)\n",
    "GRAPH = ddict()\n",
    "for name,  modules in qmodel.named_modules():\n",
    "    print(name, type(modules))\n",
    "    if isinstance(modules, Conv2d):\n",
    "        # print(f'{name} weight: {modules.weight()}')\n",
    "        # print(modules.weight().element_size())\n",
    "        # print(modules.scale, modules.zero_point)\n",
    "        # print(modules.weight().int_repr())\n",
    "        GRAPH[name]['scale'] = modules.scale\n",
    "        GRAPH[name]['zero_point'] = modules.zero_point\n",
    "        GRAPH[name]['weight']['float'] = model.state_dict()[f'{name}.weight'].detach().cpu().numpy()\n",
    "        GRAPH[name]['weight']['int'] = modules.weight().detach().cpu()#.int_repr()\n",
    "        \n",
    "        if modules.bias is not None:\n",
    "            GRAPH[name]['bias']['float'] = model.state_dict()[f'{name}.bias'].detach().cpu().numpy()\n",
    "            GRAPH[name]['bias']['qfloat'] = modules.bias().detach().cpu().numpy()\n",
    "            # GRAPH[name]['bias']['int'] = modules.bias().detach().cpu().int_repr().numpy()\n",
    "            GRAPH[name]['bias_scale'] = modules.scale\n",
    "            GRAPH[name]['bias_zero_point'] = modules.zero_point\n",
    "            # print(f'{name} bias: {modules.bias()}')\n",
    "            \n",
    "    elif isinstance(modules, Linear):\n",
    "        # print(modules.weight().element_size())\n",
    "        GRAPH[name]['weight']['int'] = modules.weight().detach().cpu()#.int_repr()\n",
    "        GRAPH[name]['weight']['float'] = model.state_dict()[f'{name}.weight'].detach().cpu().numpy()\n",
    "        GRAPH[name]['scale'] = modules.scale\n",
    "        GRAPH[name]['zero_point'] = modules.zero_point\n",
    "        # print(f'{name} weight: {modules.weight().int_repr()}')\n",
    "        if modules.bias is not None:\n",
    "            GRAPH[name]['bias']['float'] = model.state_dict()[f'{name}.bias'].detach().cpu().numpy()\n",
    "            GRAPH[name]['bias']['qfloat'] = modules.bias().detach().cpu().numpy()\n",
    "            GRAPH[name]['bias_scale'] = modules.scale\n",
    "            GRAPH[name]['bias_zero_point'] = modules.zero_point\n",
    "        #     print(f'{name} bias: {modules.bias()}')\n",
    "# print(qmodel)\n",
    "# print(get_accuracy(qmodel, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "# qweight = GRAPH['STAGE0_CONV']['weight']['int'].dequantize()[:,:3,:,:].numpy().reshape(-1)\n",
    "qweight = GRAPH['STAGE0_CONV']['weight']['int'].int_repr()[:,:3,:,:].numpy().reshape(-1)\n",
    "# print(qweight.shape)\n",
    "from fxpmath import Fxp\n",
    "weight = GRAPH['STAGE0_CONV']['weight']['float'].reshape(-1)\n",
    "# weight = np.array(Fxp(weight, signed=True, n_word=8, overflow='saturate').raw())\n",
    "\n",
    "# # print(weight.n_frac)\n",
    "scale = GRAPH['STAGE0_CONV']['scale']\n",
    "zero = GRAPH['STAGE0_CONV']['zero_point']\n",
    "print(scale, zero)\n",
    "print(weight.min(), weight.max())\n",
    "weight = (qweight)*(2**scale)\n",
    "# qweight = (qweight*scale) + zero\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "# plt.hist(qweight, bins=100, lpha=0.5, label=f'Scale:{scale}, Zero:{zero}')\n",
    "# plt.hist(weight, bins=100, color='b', alpha=0.5, label=f'Original')\n",
    "# plt.hist(qweight, bins=128, color='red')\n",
    "# plt.hist(weight, bins=128, alpha=0.5, color='blue')\n",
    "plt.scatter(np.arange(len(qweight)), qweight, c='r', label=f'Scale:{scale}, Zero:{zero}')\n",
    "plt.scatter(np.arange(len(weight)), weight, c='b', label=f'Original')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# pprint(GRAPH['LINEAR'])\n",
    "# test_qint = (GRAPH['STAGE0_CONV']['weight']['int'] - GRAPH['STAGE0_CONV']['zero_point'])*(GRAPH['STAGE0_CONV']['scale'])\n",
    "# print(test_qint)\n",
    "# test_qint2 = (GRAPH['STAGE0_CONV']['weight']['float'])\n",
    "# test_qint[:,:3, :,:] - test_qint2\n",
    "# print(GRAPH['STAGE0_CONV']['bias']['float'], GRAPH['STAGE0_CONV']['bias']['qfloat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.nn.quantized.modules.conv import Conv2d as QConv2d\n",
    "from torch.ao.nn.quantized.modules.linear import Linear as QLinear\n",
    "from torch import nn\n",
    "def hook_save_params(module, input, output):\n",
    "    setattr(module, \"input_shape\", input[0].shape)\n",
    "    setattr(module, \"output_shape\", output[0].shape)\n",
    "    setattr(module, \"input\", input[0][0])\n",
    "    setattr(module, \"output\", output[0])\n",
    "\n",
    "\n",
    "def register_hooks(model:nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (QConv2d, QLinear, nn.Conv2d, nn.Linear, nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d)):\n",
    "            module.register_forward_hook(hook_save_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_hooks(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = qmodel.to(device)\n",
    "with torch.inference_mode():\n",
    "  for img, label in testloader:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    if cnt > 10:\n",
    "        break\n",
    "    qmodel(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "q_param = OrderedDict()\n",
    "q_model = copy.deepcopy(model).cpu()\n",
    "for name, modules in model.named_modules():\n",
    "    fxp_ref = Fxp(None, signed=True, n_word=8, overflow='saturate')\n",
    "    fxp_ref.config.dtype_notation = 'Q'\n",
    "    fxp_ref.config.op_method = 'repr'\n",
    "    fxp_ref.config.op_out = Fxp(None, True, n_word=8, overflow='saturate')\n",
    "    fxp_ref.config.array_output_type = 'array'\n",
    "    if isinstance(modules, nn.Conv2d):\n",
    "        weight = modules.weight.detach().cpu().numpy()\n",
    "        q_weight = Fxp(weight, like = fxp_ref)\n",
    "        n_int = q_weight.n_int\n",
    "        n_frac = q_weight.n_frac\n",
    "        print(f'{name} n_frac: {n_frac}')\n",
    "        q_weight = q_weight << n_frac # Interger convert\n",
    "        q_weight = q_weight >> n_frac # Fixed point convert\n",
    "        \n",
    "        # q_weight = q_weight.ravel()\n",
    "        # weight = weight.ravel()\n",
    "        # plt.figure(figsize=(18,6))\n",
    "        # plt.scatter(np.arange(len(q_weight)), q_weight, c='r', label=f'Q{n_int}.{n_frac}')\n",
    "        # plt.scatter(np.arange(len(weight)), weight, c='b', label=f'Original')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        print((q_weight - weight).sum())\n",
    "        # break\n",
    "        q_model.state_dict()[f'{name}.weight'].copy_(torch.Tensor((np.array(q_weight))))\n",
    "        # print(q_model.state_dict()[f'{name}.weight'])\n",
    "        q_param[f'{name}.weight'] = (n_int, n_frac)\n",
    "        # print(torch.IntTensor(np.array(q_weight)))\n",
    "        \n",
    "        # print(q_model.state_dict()[f'{name}.weight'])\n",
    "        # model[name].weight = torch.Tensor(np.array(q_weight))\n",
    "        print(f'{name}.weight error: {(q_weight - weight).sum()}')\n",
    "        if modules.bias is not None:\n",
    "            bias = modules.bias.detach().cpu().numpy()\n",
    "            q_bias = Fxp(bias, like = fxp_ref)\n",
    "            n_int = q_bias.n_int\n",
    "            n_frac = q_bias.n_frac\n",
    "            q_bias = q_bias << n_frac\n",
    "            q_bias = q_bias >> n_frac\n",
    "            \n",
    "            q_model.state_dict()[f'{name}.bias'].copy_(torch.Tensor(np.array(q_bias)))\n",
    "            q_param[f'{name}.bias'] = (n_frac)\n",
    "            print(f'{name}.bias error: {(q_bias - bias).sum()}')\n",
    "            \n",
    "        \n",
    "    elif isinstance(modules, nn.Linear):\n",
    "        weight = modules.weight.detach().cpu().numpy()\n",
    "        q_weight = Fxp(weight, like = fxp_ref)\n",
    "        n_int = q_weight.n_int\n",
    "        n_frac = q_weight.n_frac\n",
    "        q_weight = q_weight << n_frac\n",
    "        q_weight = q_weight >> n_frac\n",
    "        \n",
    "        q_model.state_dict()[f'{name}.weight'].copy_(torch.Tensor(np.array(q_weight)))\n",
    "        q_param[f'{name}.weight'] = (n_frac)\n",
    "        print(f'{name}.weight error: {(q_weight - weight).sum()}')\n",
    "        if modules.bias is not None:\n",
    "            bias = modules.bias.detach().cpu().numpy()\n",
    "            \n",
    "            q_bias = Fxp(bias, like = fxp_ref)\n",
    "            n_int = q_bias.n_int\n",
    "            n_frac = q_bias.n_frac\n",
    "            \n",
    "            \n",
    "            q_bias = q_bias << n_frac\n",
    "            q_bias = q_bias >> n_frac\n",
    "            q_model.state_dict()[f'{name}.bias'].copy_(torch.Tensor(np.array(q_bias)))\n",
    "            q_param[f'{name}.bias'] = (n_frac)\n",
    "            print(f'{name}.bias error: {(q_bias - bias).sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fxpmath import Fxp\n",
    "for name, module in qmodel.named_modules():\n",
    "    if isinstance(module, (QConv2d, QLinear, nn.Conv2d, nn.Linear)):\n",
    "        print(module.scale)\n",
    "        fxp_scale = Fxp(module.scale, signed=True, n_word=8, overflow='saturate')\n",
    "        print(f'{name} scale: {fxp_scale.n_frac}, INT8={fxp_scale<<fxp_scale.n_frac}')\n",
    "        print(name, module.input_shape, module.output_shape, module.input.shape, module.output.shape)\n",
    "    if isinstance(module, (nn.AdaptiveAvgPool2d, nn.AvgPool2d, nn.MaxPool2d)):\n",
    "        print(name, module.input_shape, module.output_shape, module.input.shape, module.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.STAGE0_CONV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
