{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1770a2894b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# # warnings 설정\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# 반복 가능한 결과를 위한 랜덤 시드 지정하기\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"평균과 현재 값 계산 및 저장\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"특정 k값을 위해 top k 예측의 정확도 계산\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "def load_weights(model:nn.Module, model_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if checkpoint.get('state_dict'):\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCU_VGGRepC1(\n",
       "  (STAGE0_CONV): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (STAGE0_RELU): ReLU()\n",
       "  (STAGE1_0_CONV): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (STAGE1_0_RELU): ReLU()\n",
       "  (STAGE2_0_CONV): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (STAGE2_0_RELU): ReLU()\n",
       "  (STAGE3_0_CONV): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (STAGE3_0_RELU): ReLU()\n",
       "  (STAGE4_0_CONV): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (STAGE4_0_RELU): ReLU()\n",
       "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
       "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
       "  (LINEAR): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataloaders import get_dataloader\n",
    "import torch\n",
    "from torch import nn\n",
    "class MCU_VGGRepC1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCU_VGGRepC1, self).__init__()\n",
    "        self.STAGE0_CONV = nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.STAGE0_RELU = nn.ReLU()\n",
    "        self.STAGE1_0_CONV = nn.Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.STAGE1_0_RELU = nn.ReLU()\n",
    "        self.STAGE2_0_CONV = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.STAGE2_0_RELU = nn.ReLU()\n",
    "        self.STAGE3_0_CONV = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.STAGE3_0_RELU = nn.ReLU()\n",
    "        self.STAGE4_0_CONV = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.STAGE4_0_RELU = nn.ReLU()\n",
    "        self.GAP21 = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.FLATTEN22 = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.LINEAR = nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.STAGE0_CONV(x)\n",
    "        x = self.STAGE0_RELU(x)\n",
    "        x = self.STAGE1_0_CONV(x)\n",
    "        x = self.STAGE1_0_RELU(x)\n",
    "        x = self.STAGE2_0_CONV(x)\n",
    "        x = self.STAGE2_0_RELU(x)\n",
    "        x = self.STAGE3_0_CONV(x)\n",
    "        x = self.STAGE3_0_RELU(x)\n",
    "        x = self.STAGE4_0_CONV(x)\n",
    "        x = self.STAGE4_0_RELU(x)\n",
    "        x = self.GAP21(x)\n",
    "        x = self.FLATTEN22(x)\n",
    "        x = self.LINEAR(x)\n",
    "        \n",
    "        return x\n",
    "checkpoint = \"E:/2_Quantization/deployment-with-CMSIS-NN/weights/mcu_vggrepc1_vww.pth\"\n",
    "model = MCU_VGGRepC1()\n",
    "model = load_weights(model, checkpoint)\n",
    "model = model.cpu()\n",
    "recover_model = lambda : load_weights(model, checkpoint)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/1_TinyML/tiny/benchmark/training/visual_wake_words/vw_coco2014_96/'\n",
    "\n",
    "train_batch_size = 50\n",
    "eval_batch_size = 50\n",
    "\n",
    "data_loader, data_loader_test = get_dataloader(dataset_dir=data_path, \n",
    "                                batch_size=train_batch_size, \n",
    "                                image_size=96, \n",
    "                                num_workers=4, \n",
    "                                shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_weights(model, checkpoint)\n",
    "\n",
    "# 다음으로 \"모듈 결합\"을 합니다. 모듈 결합은 메모리 접근을 줄여 모델을 빠르게 만들면서\n",
    "# 정확도 수치를 향상시킵니다. 모듈 결합은 어떠한 모델에라도 사용할 수 있지만,\n",
    "# 양자화된 모델에 사용하는 것이 특히나 더 일반적입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 0.403415\n",
      "............................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 86.31\n"
     ]
    }
   ],
   "source": [
    "num_eval_batches = 1000\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "# torch.jit.save(torch.jit.script(float_model), './weights/mcu_vggrepc1_jit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCU_VGGRepC1(\n",
      "  (STAGE0_CONV): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE0_RELU): ReLU()\n",
      "  (STAGE1_0_CONV): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE1_0_RELU): ReLU()\n",
      "  (STAGE2_0_CONV): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE2_0_RELU): ReLU()\n",
      "  (STAGE3_0_CONV): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE3_0_RELU): ReLU()\n",
      "  (STAGE4_0_CONV): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE4_0_RELU): ReLU()\n",
      "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
      "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
      "  (LINEAR): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " Inverted Residual Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " MCU_VGGRepC1(\n",
      "  (STAGE0_CONV): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
      "  (STAGE0_RELU): ReLU()\n",
      "  (STAGE1_0_CONV): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
      "  (STAGE1_0_RELU): ReLU()\n",
      "  (STAGE2_0_CONV): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
      "  (STAGE2_0_RELU): ReLU()\n",
      "  (STAGE3_0_CONV): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
      "  (STAGE3_0_RELU): ReLU()\n",
      "  (STAGE4_0_CONV): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
      "  (STAGE4_0_RELU): ReLU()\n",
      "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
      "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
      "  (LINEAR): QuantizedLinear(in_features=128, out_features=2, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 0.107663\n",
      "Evaluation accuracy on 50000 images, 86.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hci-lab01\\.conda\\envs\\vai\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:310: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 32\n",
    "\n",
    "from utils.dataloaders import get_dataloader\n",
    "import torch\n",
    "from torch import nn\n",
    "checkpoint = \"E:/2_Quantization/deployment-with-CMSIS-NN/weights/mcu_vggrepc1_vww.pth\"\n",
    "myModel = MCU_VGGRepC1()\n",
    "# myModel = load_weights(model.cpu(), checkpoint)\n",
    "myModel = myModel.cpu()\n",
    "print(myModel)\n",
    "# 양자화 설정 명시\n",
    "# 간단한 min/max 범위 추정 및 텐서별 가중치 양자화로 시작\n",
    "myModel.qconfig = torch.ao.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.ao.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# 첫 번째 보정\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "\n",
    "# 학습 세트로 보정\n",
    "# evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# 양자화된 모델로 변환\n",
    "torch.ao.quantization.convert(myModel.cpu(), inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "# top1, top5 = evaluate(myModel.cpu(), criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "per_channel_quantized_model.fuse_model()\n",
    "# 이전의 'fbgemm' 또한 여전히 사용 가능하지만, 'x86'을 기본으로 사용하는 것을 권장합니다.\n",
    "per_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\n",
    "torch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = load_model(saved_model_dir + float_model_file)\n",
    "qat_model.eval()\n",
    "qat_model.fuse_model()\n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "# 이전의 'fbgemm' 또한 여전히 사용 가능하지만, 'x86'을 기본으로 사용하는 것을 권장합니다.\n",
    "qat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.train()\n",
    "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_batches = 20\n",
    "\n",
    "# QAT는 시간이 걸리는 작업이며 몇 에폭에 걸쳐 훈련이 필요합니다.\n",
    "# 학습 및 각 에폭 이후 정확도 확인\n",
    "for nepoch in range(8):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # 양자화 파라미터 고정\n",
    "        qat_model.apply(torch.ao.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # 배치 정규화 평균 및 분산 추정값 고정\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # 각 에폭 이후 정확도 확인\n",
    "    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n",
    "    quantized_model.eval()\n",
    "    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # 이미지 배치들 이용하여 스크립트된 모델 실행\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
