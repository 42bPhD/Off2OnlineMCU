{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torchsummary import summary\n",
    "import os\n",
    "from utils.dataloaders import get_dataloader, get_subnet_dataloader\n",
    "from utils.train_eval import evaluate\n",
    "from utils.functions import reconstruction_model\n",
    "from utils.io import load_weights\n",
    "\n",
    "from utils.train_eval import get_accuracy\n",
    "from utils.utils import count_net_flops\n",
    "\n",
    "from utils.bn_fold import bn_fold\n",
    "from torch import nn\n",
    "from fxpmath import Fxp\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"./weights/mcu_vggrepc1_vww.pth\"\n",
    "# model_ckpt = \"./weights/mcu_vggrepopt_cifar10.pth\"\n",
    "data_dir = \"E:/1_TinyML/tiny/benchmark/training/visual_wake_words/vw_coco2014_96\"\n",
    "image_size = 32\n",
    "workers = 4\n",
    "batch_size = 50\n",
    "from models.model_q import MCU_VGGRep, MCU_VGGRepC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCU_VGGRepC1(\n",
      "  (quant): QuantStub()\n",
      "  (STAGE0_CONV): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE0_RELU): ReLU()\n",
      "  (STAGE1_0_CONV): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE1_0_RELU): ReLU()\n",
      "  (STAGE2_0_CONV): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE2_0_RELU): ReLU()\n",
      "  (STAGE3_0_CONV): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE3_0_RELU): ReLU()\n",
      "  (STAGE4_0_CONV): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (STAGE4_0_RELU): ReLU()\n",
      "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
      "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
      "  (LINEAR): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MCU_VGGRepC1(num_classes=2)\n",
    "model = load_weights(model, model_ckpt)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flatten_model = reconstruction_model(model, device)\n",
    "def get_cifar10_loader():\n",
    "    print('=> loading cifar10 data...')\n",
    "    normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='E:/2_Quantization/torch2cmsis/examples/cifar/data/data_cifar10',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ]))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='E:/2_Quantization/torch2cmsis/examples/cifar/data/data_cifar10',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ]))\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "# trainloader, testloader = get_cifar10_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from RepOptimizers.data.dataloader import get_dataloader, get_subnet_dataloader\n",
    "data_dir = \"E:/1_TinyML/tiny/benchmark/training/visual_wake_words/vw_coco2014_96\"\n",
    "image_size = 96\n",
    "workers = 4\n",
    "batch_size = 128\n",
    "# from main import get_cifar10_loader\n",
    "trainloader, testloader = get_dataloader(data_dir, batch_size=batch_size, image_size=image_size, num_workers=workers)\n",
    "val_loader = get_subnet_dataloader(data_dir, subset_len=1000, batch_size=batch_size, image_size=image_size, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating Point Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before accuracy: 86.27% MAC+BN=4,313,472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "\n",
    "print(f\"Before accuracy: {get_accuracy(model.to(device), val_loader):.2f}%\",\n",
    "        f\"MAC+BN={count_net_flops(model, (1, 3, image_size, image_size), True):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cofing Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import get_default_qconfig, quantize_dynamic, QConfig, HistogramObserver, prepare, fuse_modules\n",
    "# Custom quantization configuration 정의\n",
    "import math\n",
    "from torch.quantization import MinMaxObserver\n",
    "import torch\n",
    "\n",
    "\n",
    "class PowerOfTwoActivationObserver(HistogramObserver):\n",
    "    def __init__(self, bits=8, *args, **kwargs):\n",
    "        super(PowerOfTwoActivationObserver, self).__init__(*args, **kwargs)\n",
    "        self.bits = bits\n",
    "        self.quant_min = 0\n",
    "        self.quant_max = 2 ** bits - 1\n",
    "        self.dtype = torch.qint8  # 추가: dtype을 torch.quint8로 설정\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0))\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        min_val, max_val = self.min_val, self.max_val\n",
    "        if min_val == max_val:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            max_val = max(abs(min_val), abs(max_val))\n",
    "            scale = max_val / ((2 ** self.bits) - 1)\n",
    "            scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "            zero_point = 0\n",
    "\n",
    "        self.scale.copy_(scale_pow2)\n",
    "        self.zero_point.copy_(zero_point)\n",
    "        return self.scale, self.zero_point\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.min_val = min(self.min_val, x.min())\n",
    "        self.max_val = max(self.max_val, x.max())\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"min_val={}, max_val={}, scale={}, zero_point={}, bits={}\".format(\n",
    "            self.min_val, self.max_val, self.scale, self.zero_point, self.bits\n",
    "        )\n",
    "class PowerOfTwoWeightObserver(HistogramObserver):\n",
    "    def __init__(self, bits=8, *args, **kwargs):\n",
    "        super(PowerOfTwoWeightObserver, self).__init__(*args, **kwargs)\n",
    "        self.bits = bits\n",
    "        self.dtype = torch.qint8  # 추가: dtype을 torch.quint8로 설정\n",
    "\n",
    "        self.quant_min = -2 ** (bits - 1)\n",
    "        self.quant_max = 2 ** (bits - 1) - 1\n",
    "        self.register_buffer('scale', torch.tensor(1.0))\n",
    "        self.register_buffer('zero_point', torch.tensor(0))\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        min_val, max_val = self.min_val, self.max_val\n",
    "        if min_val == max_val:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            max_val = max(abs(min_val), abs(max_val))\n",
    "            scale = max_val / (2 ** (self.bits - 1) - 1)\n",
    "            scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "            zero_point = 0\n",
    "\n",
    "        self.scale.copy_(scale_pow2)\n",
    "        self.zero_point.copy_(zero_point)\n",
    "        return self.scale, self.zero_point\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.min_val = min(self.min_val, x.min())\n",
    "        self.max_val = max(self.max_val, x.max())\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"min_val={}, max_val={}, scale={}, zero_point={}, bits={}\".format(\n",
    "            self.min_val, self.max_val, self.scale, self.zero_point, self.bits\n",
    "        )\n",
    "\n",
    "backend = \"x86\"\n",
    "model.qconfig = QConfig(\n",
    "            activation=PowerOfTwoWeightObserver.with_args(bits=8, \n",
    "                                                          qscheme=torch.per_tensor_symmetric,\n",
    "                                                          dtype=torch.qint8),\n",
    "            weight=PowerOfTwoWeightObserver.with_args(bits=8,\n",
    "                                              qscheme=torch.per_tensor_symmetric,\n",
    "                                              dtype=torch.qint8)\n",
    "            )\n",
    "fuse_modules(model, [['STAGE0_CONV', 'STAGE0_RELU'],\n",
    "                     ['STAGE1_0_CONV', 'STAGE1_0_RELU'],\n",
    "                     ['STAGE2_0_CONV', 'STAGE2_0_RELU'],\n",
    "                     ['STAGE3_0_CONV', 'STAGE3_0_RELU'],\n",
    "                     ['STAGE4_0_CONV', 'STAGE4_0_RELU']], inplace=True)\n",
    "# model.qconfig = torch.quantization.QConfig(\n",
    "#     activation=HistogramObserver.with_args(dtype=torch.qint8, \n",
    "#                                            qscheme=torch.per_tensor_symmetric),\n",
    "#     weight=HistogramObserver.with_args(dtype=torch.qint8,\n",
    "#                                         qscheme=torch.per_tensor_symmetric)\n",
    "# )\n",
    "qmodel = prepare(model, inplace=False)\n",
    "# model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "# torch.backends.quantized.engine = backend\n",
    "# model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "# model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n",
    "# model.qconfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCU_VGGRepC1(\n",
       "  (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
       "  (STAGE0_CONV): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE0_RELU): Identity()\n",
       "  (STAGE1_0_CONV): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "  (STAGE1_0_RELU): Identity()\n",
       "  (STAGE2_0_CONV): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.25, zero_point=0, padding=(1, 1))\n",
       "  (STAGE2_0_RELU): Identity()\n",
       "  (STAGE3_0_CONV): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.25, zero_point=0, padding=(1, 1))\n",
       "  (STAGE3_0_RELU): Identity()\n",
       "  (STAGE4_0_CONV): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.125, zero_point=0, padding=(1, 1))\n",
       "  (STAGE4_0_RELU): Identity()\n",
       "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
       "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
       "  (LINEAR): QuantizedLinear(in_features=128, out_features=2, scale=0.015625, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "qmodel = qmodel.to(device)\n",
    "with torch.inference_mode():\n",
    "    for img, label in trainloader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        if cnt > 10:\n",
    "            break\n",
    "        qmodel(img)\n",
    "    \n",
    "qmodel = torch.quantization.convert(qmodel, inplace=True)\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.78031158447266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(qmodel, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "imgs = iter(testloader).__next__()[0]\n",
    "print(imgs.shape)\n",
    "_ = qmodel(imgs.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404.34 KB\n",
      "108.69 KB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f KB\" %(os.path.getsize(\"tmp.pt\")/1e3))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1406, -0.1406]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = imgs[0]\n",
    "img.shape\n",
    "qmodel(img.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1406, -0.1406]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from fxpmath import Fxp\n",
    "import numpy as np\n",
    "\n",
    "def qfmt_quanize(x, n_bits=8, signed=True):\n",
    "    range_min, range_max = torch.min(x), torch.max(x)\n",
    "    range_abs = torch.max(torch.abs(range_min), torch.abs(range_max))\n",
    "    int_bits = torch.ceil(torch.log2(range_abs)).type(torch.int8)\n",
    "    frac_bits = n_bits - int_bits\n",
    "    if signed:\n",
    "        range_int_min = -(2 ** n_bits)\n",
    "        range_int_max = (2 ** n_bits) - 1\n",
    "        \n",
    "        # frac_bits = 7 if frac_bits >= 8 else frac_bits - 1\n",
    "        frac_bits -= 1\n",
    "    else:\n",
    "        range_int_min = 0\n",
    "        range_int_max = (2 ** n_bits)\n",
    "    # Quantization the input\n",
    "    \n",
    "    x_int = torch.round(x * (2 ** (frac_bits))).to(torch.int8)\n",
    "    x_float = torch.clamp(x_int * (1/(2 ** (frac_bits))), range_int_min, range_int_max)\n",
    "    # quant_error = torch.mean((x - x_float) ** 2)\n",
    "    frac_bits = frac_bits if isinstance(frac_bits, int) else frac_bits.item()\n",
    "    return x_int, frac_bits\n",
    "class HookRecorder:\n",
    "    def __init__(self):\n",
    "        self.recorder = dict() # Get intermediate tensor from the recorder\n",
    "        self.handlers = list()\n",
    "    \n",
    "    def _register_hooker(self, name):\n",
    "        self.recorder[name] = dict()\n",
    "        def named_hooker(module, input: Tuple[torch.Tensor], output: torch.Tensor):\n",
    "            input = input[0].dequantize().detach().cpu().numpy()\n",
    "            input = Fxp(input, signed=True, n_word=8, overflow='saturate')\n",
    "            x_frac = input.n_frac\n",
    "            x_int = torch.tensor(np.array(input << input.n_frac).astype(np.int8))\n",
    "            # x_int, x_frac = qfmt_quanize(input[0], 8, True)\n",
    "            \n",
    "            self.recorder[name][\"input\"] = x_int\n",
    "            self.recorder[name][\"i_f\"] = x_frac\n",
    "            self.recorder[name]['input_shape'] = x_int.shape\n",
    "            \n",
    "            output = output.dequantize().detach().cpu().numpy()\n",
    "            output = Fxp(output, signed=True, n_word=8, overflow='saturate')\n",
    "            y_frac = output.n_frac\n",
    "            y_int = torch.tensor(np.array(output << output.n_frac).astype(np.int8))\n",
    "            # y_int, y_frac = qfmt_quanize(output, 8, True)\n",
    "            self.recorder[name][\"output\"] = y_int\n",
    "            self.recorder[name][\"o_f\"] = y_frac\n",
    "            self.recorder[name]['output_shape'] = y_int.shape\n",
    "            \n",
    "            \n",
    "        return named_hooker\n",
    "    \n",
    "    def register_hookers(self, target_sub_modules, layer_names):\n",
    "        for i in range(len(layer_names)):\n",
    "            module = target_sub_modules[i]\n",
    "            layer_name = layer_names[i]\n",
    "            handler = module.register_forward_hook(self._register_hooker(layer_name))\n",
    "        self.handlers.append(handler)\n",
    "        \n",
    "    def remove_handlers(self):\n",
    "        for i in self.handlers:\n",
    "            i.remove()\n",
    "        self.handlers.clear()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.remove_handlers()\n",
    "\n",
    "hook = HookRecorder()\n",
    "hook.register_hookers([qmodel.quant, qmodel.STAGE0_CONV, qmodel.STAGE1_0_CONV, \n",
    "                       qmodel.STAGE2_0_CONV, qmodel.STAGE3_0_CONV, \n",
    "                       qmodel.STAGE4_0_CONV, qmodel.LINEAR], \n",
    "                      [\"quant\", \"STAGE0_CONV\", \"STAGE1_0_CONV\", \n",
    "                       \"STAGE2_0_CONV\", \"STAGE3_0_CONV\",\n",
    "                       \"STAGE4_0_CONV\", \"LINEAR\"])\n",
    "\n",
    "qmodel(img.unsqueeze(0).to(device))\n",
    "# print(hook.recorder)\n",
    "# hook.remove_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['quant', 'STAGE0_CONV', 'STAGE1_0_CONV', 'STAGE2_0_CONV', 'STAGE3_0_CONV', 'STAGE4_0_CONV', 'LINEAR'])\n",
      "torch.Size([1, 3, 96, 96])\n",
      "torch.Size([1, 16, 48, 48])\n",
      "6\n",
      "1\n",
      "dict_keys(['input', 'i_f', 'input_shape', 'output', 'o_f', 'output_shape'])\n"
     ]
    }
   ],
   "source": [
    "print(hook.recorder.keys())\n",
    "print(hook.recorder['STAGE0_CONV']['input_shape'])\n",
    "print(hook.recorder['STAGE0_CONV']['output_shape'])\n",
    "print(hook.recorder['STAGE0_CONV']['i_f'])\n",
    "print(hook.recorder['STAGE0_CONV']['o_f'])\n",
    "print(hook.recorder['STAGE0_CONV'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-18, -11,  -7],\n",
      "        [ 21,  40,  27],\n",
      "        [ 19,  19,   7]], device='cuda:0', dtype=torch.int8)\n",
      "[[-18. -11.  -7.]\n",
      " [ 21.  40.  27.]\n",
      " [ 19.  19.   7.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fxpmath import Fxp\n",
    "\n",
    "def simulate(img, qmodel, verbose=False):\n",
    "    quant = qmodel.quant.scale.item()\n",
    "    inp = torch.round(img*(1/quant)).int()\n",
    "    \n",
    "    \"\"\"\n",
    "    (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
    "    (STAGE0_CONV): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
    "    (STAGE1_0_CONV): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
    "    (STAGE2_0_CONV): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
    "    (STAGE3_0_CONV): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
    "    (STAGE4_0_CONV): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.0625, zero_point=0, padding=(1, 1))\n",
    "    (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
    "    (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
    "    (LINEAR): QuantizedLinear(in_features=128, out_features=10, scale=0.03125, zero_point=0, qscheme=torch.per_tensor_affine)\n",
    "    (dequant): DeQuantize()\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(inp)\n",
    "    qmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        qmodel = qmodel.to(device)\n",
    "        \n",
    "        w = qmodel.STAGE0_CONV.weight().int_repr()[:3, 0, 0, :]\n",
    "        fw = qmodel.STAGE0_CONV.weight().dequantize()[:3, 0, 0, :].detach().cpu().numpy()\n",
    "        print(w)\n",
    "        # fw = fw*(1/scale)\n",
    "        # print(fw)\n",
    "        fw = Fxp(fw, signed=True, n_word=8)\n",
    "        print(fw<<fw.n_frac)\n",
    "        # scale_pow2 = 2 ** torch.floor(torch.log2(scale))\n",
    "        \n",
    "    #     # print(model.STAGE0_CONV.weight().dequantize()[:, :3, :,:])\n",
    "    #     print(w, fw.raw())\n",
    "    #     # print(model.STAGE0_CONV.bias().dequantize()*(1/scale))\n",
    "    # return pow2s\n",
    "        \n",
    "qmodel  \n",
    "pow2s = simulate(img, qmodel)\n",
    "# print(pow2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCU_VGGRepC1(\n",
       "  (quant): Quantize(scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0'), dtype=torch.qint8)\n",
       "  (STAGE0_CONV): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.5, zero_point=0, padding=(1, 1))\n",
       "  (STAGE0_RELU): Identity()\n",
       "  (STAGE1_0_CONV): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "  (STAGE1_0_RELU): Identity()\n",
       "  (STAGE2_0_CONV): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.25, zero_point=0, padding=(1, 1))\n",
       "  (STAGE2_0_RELU): Identity()\n",
       "  (STAGE3_0_CONV): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.25, zero_point=0, padding=(1, 1))\n",
       "  (STAGE3_0_RELU): Identity()\n",
       "  (STAGE4_0_CONV): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.125, zero_point=0, padding=(1, 1))\n",
       "  (STAGE4_0_RELU): Identity()\n",
       "  (GAP21): AdaptiveAvgPool2d(output_size=1)\n",
       "  (FLATTEN22): Flatten(start_dim=1, end_dim=-1)\n",
       "  (LINEAR): QuantizedLinear(in_features=128, out_features=2, scale=0.015625, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.qconfig.activation().calculate_qparams()\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extra_preprocess(x:torch.Tensor):\n",
    "    # hint: you need to convert the original fp32 input of range (0, 1)\n",
    "    #  into int8 format of range (-128, 127)\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    from fxpmath import Fxp\n",
    "    import numpy as np\n",
    "    np_x = x.numpy()\n",
    "    x = torch.tensor(np.array(Fxp(np_x, signed=True, n_word=8).raw(), dtype=np.int8))\n",
    "    return x.to(torch.int8)\n",
    "    ############### YOUR CODE ENDS HERE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m qmodel \u001b[38;5;241m=\u001b[39m qmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     56\u001b[0m plain_model \u001b[38;5;241m=\u001b[39m plain_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_preprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_process\u001b[49m\u001b[43m)\u001b[49m, get_accuracy(qmodel, testloader))\n",
      "File \u001b[1;32mc:\\Users\\hci-lab01\\.conda\\envs\\lab\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\2_Quantization\\Off2OnlineMCU\\Model_Exp\\off_line\\1_RepVGG\\utils\\train_eval.py:117\u001b[0m, in \u001b[0;36mget_accuracy\u001b[1;34m(model, dataloader, extra_preprocess)\u001b[0m\n\u001b[0;32m    115\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m preprocess \u001b[38;5;129;01min\u001b[39;00m extra_preprocess:\n\u001b[0;32m    118\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m preprocess(inputs)\n\u001b[0;32m    120\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "plain_model = MCU_VGGRepC1(num_classes=10, quant=False)\n",
    "qmodel = qmodel.cpu()\n",
    "plain_model = plain_model.cpu()\n",
    "quantized_state_dict = qmodel.state_dict()\n",
    "state_dict = plain_model.state_dict()\n",
    "\n",
    "def qfmt_quanize(x, n_bits=8, signed=True):\n",
    "    range_min, range_max = torch.min(x), torch.max(x)\n",
    "    range_abs = torch.max(torch.abs(range_min), torch.abs(range_max))\n",
    "    int_bits = torch.ceil(torch.log2(range_abs)).type(torch.int8)\n",
    "    frac_bits = n_bits - int_bits\n",
    "    if signed:\n",
    "        range_int_min = -(2 ** n_bits)\n",
    "        range_int_max = (2 ** n_bits) - 1\n",
    "        \n",
    "        # frac_bits = 7 if frac_bits >= 8 else frac_bits - 1\n",
    "        frac_bits -= 1\n",
    "    else:\n",
    "        range_int_min = 0\n",
    "        range_int_max = (2 ** n_bits)\n",
    "    # Quantization the input\n",
    "    \n",
    "    x_int = torch.round(x * (2 ** (frac_bits))).to(torch.int8)\n",
    "    x_float = torch.clamp(x_int * (1/(2 ** (frac_bits))), range_int_min, range_int_max)\n",
    "    # quant_error = torch.mean((x - x_float) ** 2)\n",
    "    frac_bits = frac_bits if isinstance(frac_bits, int) else frac_bits.item()\n",
    "    return x_float, frac_bits\n",
    "\n",
    "def input_process(inputs):\n",
    "    \n",
    "    return qfmt_quanize(inputs, 8, True)[0]\n",
    "    \n",
    "# 가중치와 바이어스 복사\n",
    "for name, param in state_dict.items():\n",
    "    if name in quantized_state_dict:\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            # Quantization된 모델의 가중치/바이어스 텐서 가져오기\n",
    "            quantized_param = quantized_state_dict[name]\n",
    "            \n",
    "            dequantized_param = quantized_param.dequantize()\n",
    "            # print(dequantized_param.size(), param.size())\n",
    "            # 첫 번째 차원 크기 비교\n",
    "            if dequantized_param.dim()>1 and dequantized_param.size(1) != param.size(1):\n",
    "                if dequantized_param.size(1) == param.size(1) + 1:\n",
    "                    \n",
    "                    dequantized_param = dequantized_param[:, :-1]  # 첫 번째 차원의 크기가 1 더 크면 첫 번째 채널 제거\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected size mismatch in {name}: {dequantized_param.size()} vs {param.size()}\")\n",
    "            # 크기 조정\n",
    "            if dequantized_param.size() != param.size():\n",
    "                dequantized_param = dequantized_param.view(param.size())\n",
    "            \n",
    "            # 가중치/바이어스 복사\n",
    "            param.data.copy_(dequantized_param)\n",
    "qmodel = qmodel.to(device)\n",
    "plain_model = plain_model.to(device)\n",
    "print(get_accuracy(plain_model, testloader, extra_preprocess=input_process), get_accuracy(qmodel, testloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.train_eval import AverageMeter, ProgressMeter, accuracy\n",
    "import time\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_Int_accuracy(model: nn.Module,\n",
    "                    dataloader: DataLoader,\n",
    "                    extra_preprocess = None,\n",
    "                    device:str = 'cuda:0') -> float:\n",
    "    model.eval()\n",
    "    \n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "        # Move the data from CPU to GPU\n",
    "        inputs = inputs.cpu()    \n",
    "        if extra_preprocess is not None:\n",
    "            for preprocess in extra_preprocess:\n",
    "                inputs = preprocess(inputs)\n",
    "\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "    return (num_correct / num_samples * 100).item()\n",
    "\n",
    "def evaluate(dataloader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(dataloader), [batch_time, losses, top1, top5], prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(dataloader):\n",
    "            model = model.cuda()\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(\n",
    "            top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg\n",
    "    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = qmodel.cuda()\n",
    "qmodel.STAGE0_CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qmodel.STAGE0_CONV.weight().int_repr()[:, :3, :, :]\n",
    "# print(qmodel.LINEAR.weight().detach().cpu())\n",
    "# w = qmodel.LINEAR.weight().detach().cpu()\n",
    "# (1/qmodel.LINEAR.scale)\n",
    "# print(w.data.int_repr())\n",
    "import math\n",
    "from torch.quantization import get_observer_state_dict\n",
    "# qmodel(next(iter(trainloader))[0])\n",
    "# w.dequantize()\n",
    "observer_dict = get_observer_state_dict(qmodel)\n",
    "print(observer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.nn.quantized.modules.conv import Conv2d\n",
    "from torch.ao.nn.quantized.modules.linear import Linear\n",
    "qmodel = qmodel.to(device)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def ddict():\n",
    "    return defaultdict(ddict)\n",
    "GRAPH = ddict()\n",
    "for name,  modules in qmodel.named_modules():\n",
    "    print(name, type(modules))\n",
    "    if isinstance(modules, Conv2d):\n",
    "        # print(f'{name} weight: {modules.weight()}')\n",
    "        # print(modules.weight().element_size())\n",
    "        # print(modules.scale, modules.zero_point)\n",
    "        # print(modules.weight().int_repr())\n",
    "        GRAPH[name]['scale'] = modules.scale\n",
    "        GRAPH[name]['zero_point'] = modules.zero_point\n",
    "        GRAPH[name]['weight']['float'] = model.state_dict()[f'{name}.weight'].detach().cpu().numpy()\n",
    "        GRAPH[name]['weight']['int'] = modules.weight().detach().cpu()#.int_repr()\n",
    "        \n",
    "        if modules.bias is not None:\n",
    "            GRAPH[name]['bias']['float'] = model.state_dict()[f'{name}.bias'].detach().cpu().numpy()\n",
    "            GRAPH[name]['bias']['qfloat'] = modules.bias().detach().cpu().numpy()\n",
    "            # GRAPH[name]['bias']['int'] = modules.bias().detach().cpu().int_repr().numpy()\n",
    "            GRAPH[name]['bias_scale'] = modules.scale\n",
    "            GRAPH[name]['bias_zero_point'] = modules.zero_point\n",
    "            # print(f'{name} bias: {modules.bias()}')\n",
    "            \n",
    "    elif isinstance(modules, Linear):\n",
    "        # print(modules.weight().element_size())\n",
    "        GRAPH[name]['weight']['int'] = modules.weight().detach().cpu()#.int_repr()\n",
    "        GRAPH[name]['weight']['float'] = model.state_dict()[f'{name}.weight'].detach().cpu().numpy()\n",
    "        GRAPH[name]['scale'] = modules.scale\n",
    "        GRAPH[name]['zero_point'] = modules.zero_point\n",
    "        # print(f'{name} weight: {modules.weight().int_repr()}')\n",
    "        if modules.bias is not None:\n",
    "            GRAPH[name]['bias']['float'] = model.state_dict()[f'{name}.bias'].detach().cpu().numpy()\n",
    "            GRAPH[name]['bias']['qfloat'] = modules.bias().detach().cpu().numpy()\n",
    "            GRAPH[name]['bias_scale'] = modules.scale\n",
    "            GRAPH[name]['bias_zero_point'] = modules.zero_point\n",
    "        #     print(f'{name} bias: {modules.bias()}')\n",
    "# print(qmodel)\n",
    "# print(get_accuracy(qmodel, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "# qweight = GRAPH['STAGE0_CONV']['weight']['int'].dequantize()[:,:3,:,:].numpy().reshape(-1)\n",
    "qweight = GRAPH['STAGE0_CONV']['weight']['int'].int_repr()[:,:3,:,:].numpy().reshape(-1)\n",
    "# print(qweight.shape)\n",
    "from fxpmath import Fxp\n",
    "weight = GRAPH['STAGE0_CONV']['weight']['float'].reshape(-1)\n",
    "# weight = np.array(Fxp(weight, signed=True, n_word=8, overflow='saturate').raw())\n",
    "\n",
    "# # print(weight.n_frac)\n",
    "scale = GRAPH['STAGE0_CONV']['scale']\n",
    "zero = GRAPH['STAGE0_CONV']['zero_point']\n",
    "print(scale, zero)\n",
    "print(weight.min(), weight.max())\n",
    "weight = (qweight)*(2**scale)\n",
    "# qweight = (qweight*scale) + zero\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "# plt.hist(qweight, bins=100, lpha=0.5, label=f'Scale:{scale}, Zero:{zero}')\n",
    "# plt.hist(weight, bins=100, color='b', alpha=0.5, label=f'Original')\n",
    "# plt.hist(qweight, bins=128, color='red')\n",
    "# plt.hist(weight, bins=128, alpha=0.5, color='blue')\n",
    "plt.scatter(np.arange(len(qweight)), qweight, c='r', label=f'Scale:{scale}, Zero:{zero}')\n",
    "plt.scatter(np.arange(len(weight)), weight, c='b', label=f'Original')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# pprint(GRAPH['LINEAR'])\n",
    "# test_qint = (GRAPH['STAGE0_CONV']['weight']['int'] - GRAPH['STAGE0_CONV']['zero_point'])*(GRAPH['STAGE0_CONV']['scale'])\n",
    "# print(test_qint)\n",
    "# test_qint2 = (GRAPH['STAGE0_CONV']['weight']['float'])\n",
    "# test_qint[:,:3, :,:] - test_qint2\n",
    "# print(GRAPH['STAGE0_CONV']['bias']['float'], GRAPH['STAGE0_CONV']['bias']['qfloat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.nn.quantized.modules.conv import Conv2d as QConv2d\n",
    "from torch.ao.nn.quantized.modules.linear import Linear as QLinear\n",
    "from torch import nn\n",
    "def hook_save_params(module, input, output):\n",
    "    setattr(module, \"input_shape\", input[0].shape)\n",
    "    setattr(module, \"output_shape\", output[0].shape)\n",
    "    setattr(module, \"input\", input[0][0])\n",
    "    setattr(module, \"output\", output[0])\n",
    "\n",
    "\n",
    "def register_hooks(model:nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (QConv2d, QLinear, nn.Conv2d, nn.Linear, nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d)):\n",
    "            module.register_forward_hook(hook_save_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_hooks(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = qmodel.to(device)\n",
    "with torch.inference_mode():\n",
    "  for img, label in testloader:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    if cnt > 10:\n",
    "        break\n",
    "    qmodel(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "q_param = OrderedDict()\n",
    "q_model = copy.deepcopy(model).cpu()\n",
    "for name, modules in model.named_modules():\n",
    "    fxp_ref = Fxp(None, signed=True, n_word=8, overflow='saturate')\n",
    "    fxp_ref.config.dtype_notation = 'Q'\n",
    "    fxp_ref.config.op_method = 'repr'\n",
    "    fxp_ref.config.op_out = Fxp(None, True, n_word=8, overflow='saturate')\n",
    "    fxp_ref.config.array_output_type = 'array'\n",
    "    if isinstance(modules, nn.Conv2d):\n",
    "        weight = modules.weight.detach().cpu().numpy()\n",
    "        q_weight = Fxp(weight, like = fxp_ref)\n",
    "        n_int = q_weight.n_int\n",
    "        n_frac = q_weight.n_frac\n",
    "        print(f'{name} n_frac: {n_frac}')\n",
    "        q_weight = q_weight << n_frac # Interger convert\n",
    "        q_weight = q_weight >> n_frac # Fixed point convert\n",
    "        \n",
    "        # q_weight = q_weight.ravel()\n",
    "        # weight = weight.ravel()\n",
    "        # plt.figure(figsize=(18,6))\n",
    "        # plt.scatter(np.arange(len(q_weight)), q_weight, c='r', label=f'Q{n_int}.{n_frac}')\n",
    "        # plt.scatter(np.arange(len(weight)), weight, c='b', label=f'Original')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        print((q_weight - weight).sum())\n",
    "        # break\n",
    "        q_model.state_dict()[f'{name}.weight'].copy_(torch.Tensor((np.array(q_weight))))\n",
    "        # print(q_model.state_dict()[f'{name}.weight'])\n",
    "        q_param[f'{name}.weight'] = (n_int, n_frac)\n",
    "        # print(torch.IntTensor(np.array(q_weight)))\n",
    "        \n",
    "        # print(q_model.state_dict()[f'{name}.weight'])\n",
    "        # model[name].weight = torch.Tensor(np.array(q_weight))\n",
    "        print(f'{name}.weight error: {(q_weight - weight).sum()}')\n",
    "        if modules.bias is not None:\n",
    "            bias = modules.bias.detach().cpu().numpy()\n",
    "            q_bias = Fxp(bias, like = fxp_ref)\n",
    "            n_int = q_bias.n_int\n",
    "            n_frac = q_bias.n_frac\n",
    "            q_bias = q_bias << n_frac\n",
    "            q_bias = q_bias >> n_frac\n",
    "            \n",
    "            q_model.state_dict()[f'{name}.bias'].copy_(torch.Tensor(np.array(q_bias)))\n",
    "            q_param[f'{name}.bias'] = (n_frac)\n",
    "            print(f'{name}.bias error: {(q_bias - bias).sum()}')\n",
    "            \n",
    "        \n",
    "    elif isinstance(modules, nn.Linear):\n",
    "        weight = modules.weight.detach().cpu().numpy()\n",
    "        q_weight = Fxp(weight, like = fxp_ref)\n",
    "        n_int = q_weight.n_int\n",
    "        n_frac = q_weight.n_frac\n",
    "        q_weight = q_weight << n_frac\n",
    "        q_weight = q_weight >> n_frac\n",
    "        \n",
    "        q_model.state_dict()[f'{name}.weight'].copy_(torch.Tensor(np.array(q_weight)))\n",
    "        q_param[f'{name}.weight'] = (n_frac)\n",
    "        print(f'{name}.weight error: {(q_weight - weight).sum()}')\n",
    "        if modules.bias is not None:\n",
    "            bias = modules.bias.detach().cpu().numpy()\n",
    "            \n",
    "            q_bias = Fxp(bias, like = fxp_ref)\n",
    "            n_int = q_bias.n_int\n",
    "            n_frac = q_bias.n_frac\n",
    "            \n",
    "            \n",
    "            q_bias = q_bias << n_frac\n",
    "            q_bias = q_bias >> n_frac\n",
    "            q_model.state_dict()[f'{name}.bias'].copy_(torch.Tensor(np.array(q_bias)))\n",
    "            q_param[f'{name}.bias'] = (n_frac)\n",
    "            print(f'{name}.bias error: {(q_bias - bias).sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fxpmath import Fxp\n",
    "for name, module in qmodel.named_modules():\n",
    "    if isinstance(module, (QConv2d, QLinear, nn.Conv2d, nn.Linear)):\n",
    "        print(module.scale)\n",
    "        fxp_scale = Fxp(module.scale, signed=True, n_word=8, overflow='saturate')\n",
    "        print(f'{name} scale: {fxp_scale.n_frac}, INT8={fxp_scale<<fxp_scale.n_frac}')\n",
    "        print(name, module.input_shape, module.output_shape, module.input.shape, module.output.shape)\n",
    "    if isinstance(module, (nn.AdaptiveAvgPool2d, nn.AvgPool2d, nn.MaxPool2d)):\n",
    "        print(name, module.input_shape, module.output_shape, module.input.shape, module.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.STAGE0_CONV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
